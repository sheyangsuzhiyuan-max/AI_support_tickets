{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning\n",
    "\n",
    "This notebook implements BERT fine-tuning for text classification using HuggingFace Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root to Python path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_scheduler\n",
    "import numpy as np\n",
    "from src.data_utils import load_text_classification_data\n",
    "from src.text_preprocess import basic_clean\n",
    "from src.model.bert_model import BertClassifier, get_tokenizer\n",
    "from src.train_nn import train_epoch_with_scheduler, eval_epoch_bert\n",
    "from src.evaluate import evaluate_classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 19782\n",
      "Validation samples: 4239\n",
      "Test samples: 4240\n",
      "Label mapping: {'high': 0, 'low': 1, 'medium': 2}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_texts, train_labels, label2id, id2label = load_text_classification_data('train')\n",
    "val_texts, val_labels, _, _ = load_text_classification_data('val')\n",
    "test_texts, test_labels, _, _ = load_text_classification_data('test')\n",
    "\n",
    "# BERT 使用原始文本，仅去掉首尾空白；TF-IDF/CNN 仍可单独使用 basic_clean\n",
    "train_texts_clean = [text.strip() for text in train_texts]\n",
    "val_texts_clean = [text.strip() for text in val_texts]\n",
    "test_texts_clean = [text.strip() for text in test_texts]\n",
    "\n",
    "print(f\"Training samples: {len(train_texts_clean)}\")\n",
    "print(f\"Validation samples: {len(val_texts_clean)}\")\n",
    "print(f\"Test samples: {len(test_texts_clean)}\")\n",
    "print(f\"Label mapping: {label2id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: distilbert-base-uncased\n",
      "Max length: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"/root/AI_support_tickets/src/model/distilbert-base-uncased\"  # 用上一步确认过的那个\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "max_length = 512  # 修改: 从256提升到512以包含更多文本信息（平均文本长度411字符）\n",
    "\n",
    "print(f\"Tokenizer loaded: {model_name}\")\n",
    "print(f\"Max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    \"\"\"Dataset for BERT text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):  # 修改: 默认值从256改为512\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove batch dimension: (1, seq_len) -> (seq_len,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # Remove batch dimension\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1237\n",
      "Val batches: 265\n",
      "Test batches: 265\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = BertDataset(train_texts_clean, train_labels, tokenizer, max_length=max_length)\n",
    "val_dataset = BertDataset(val_texts_clean, val_labels, tokenizer, max_length=max_length)\n",
    "test_dataset = BertDataset(test_texts_clean, test_labels, tokenizer, max_length=max_length)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 66,365,187\n",
      "Trainable parameters: 66,365,187\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT classifier\n",
    "model = BertClassifier(\n",
    "    model_name=model_path,\n",
    "    num_classes=3,\n",
    "    dropout=0.3,\n",
    "    freeze_bert=False\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs\n",
      "Total steps: 12370\n",
      "Warmup steps: 1237\n",
      "Initial learning rate: 0.00e+00\n",
      "Class weights: [0.8565860986709595, 1.630967140197754, 0.8200472593307495]\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer and loss\n",
    "# 修改: 降低学习率从5e-5到2e-5，使用更稳定的训练\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = (len(train_labels) / (len(class_counts) * class_counts)).astype(np.float32)\n",
    "class_weights_tensor = torch.tensor(class_weights, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Setup learning rate scheduler with warmup\n",
    "# 修改: 增加训练轮数从3到10，使用cosine调度器\n",
    "num_epochs = 10\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\",  # 修改: 从linear改为cosine，避免学习率过早归零\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs\")\n",
    "print(f\"Total steps: {num_training_steps}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "print(f\"Class weights: {class_weights.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Training Loop with Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions are now imported from src.train_nn\n",
    "# train_epoch_with_scheduler and eval_epoch_bert are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Early Stopping (patience=3)...\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.0853, Train Acc: 0.3838\n",
      "  Val Loss: 1.0740, Val Acc: 0.4588\n",
      "  Learning Rate: 2.00e-05\n",
      "  ✓ New best validation accuracy: 0.4588\n",
      "\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.9829, Train Acc: 0.4906\n",
      "  Val Loss: 0.9488, Val Acc: 0.5188\n",
      "  Learning Rate: 1.94e-05\n",
      "  ✓ New best validation accuracy: 0.5188\n",
      "\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.7683, Train Acc: 0.6411\n",
      "  Val Loss: 0.9079, Val Acc: 0.6112\n",
      "  Learning Rate: 1.77e-05\n",
      "  ✓ New best validation accuracy: 0.6112\n",
      "\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.5464, Train Acc: 0.7621\n",
      "  Val Loss: 0.8580, Val Acc: 0.6719\n",
      "  Learning Rate: 1.50e-05\n",
      "  ✓ New best validation accuracy: 0.6719\n",
      "\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.3686, Train Acc: 0.8453\n",
      "  Val Loss: 0.9080, Val Acc: 0.7087\n",
      "  Learning Rate: 1.17e-05\n",
      "  ✓ New best validation accuracy: 0.7087\n",
      "\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.2364, Train Acc: 0.9030\n",
      "  Val Loss: 1.0805, Val Acc: 0.7318\n",
      "  Learning Rate: 8.26e-06\n",
      "  ✓ New best validation accuracy: 0.7318\n",
      "\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.1573, Train Acc: 0.9387\n",
      "  Val Loss: 1.2688, Val Acc: 0.7445\n",
      "  Learning Rate: 5.00e-06\n",
      "  ✓ New best validation accuracy: 0.7445\n",
      "\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.1103, Train Acc: 0.9585\n",
      "  Val Loss: 1.3475, Val Acc: 0.7481\n",
      "  Learning Rate: 2.34e-06\n",
      "  ✓ New best validation accuracy: 0.7481\n",
      "\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.0799, Train Acc: 0.9705\n",
      "  Val Loss: 1.4355, Val Acc: 0.7495\n",
      "  Learning Rate: 6.03e-07\n",
      "  ✓ New best validation accuracy: 0.7495\n",
      "\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.0681, Train Acc: 0.9751\n",
      "  Val Loss: 1.4363, Val Acc: 0.7521\n",
      "  Learning Rate: 0.00e+00\n",
      "  ✓ New best validation accuracy: 0.7521\n",
      "\n",
      "\n",
      "✓ Loaded best model with validation accuracy: 0.7521\n"
     ]
    }
   ],
   "source": [
    "# Training loop with Early Stopping\n",
    "best_val_acc = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting training with Early Stopping (patience=3)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch_with_scheduler(\n",
    "        train_loader, model, criterion, optimizer, scheduler, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = eval_epoch_bert(val_loader, model, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Early Stopping logic\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"  ✓ New best validation accuracy: {best_val_acc:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  - No improvement (patience: {patience_counter}/{patience})\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n⚠ Early stopping triggered at epoch {epoch+1}\")\n",
    "            print(f\"Best validation accuracy: {best_val_acc:.4f} (loss: {best_val_loss:.4f})\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n✓ Loaded best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No improvement found, using final model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "Accuracy: 0.7521\n",
      "F1 Macro: 0.7466\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1615\n",
      "           1       0.78      0.67      0.72       855\n",
      "           2       0.73      0.78      0.75      1769\n",
      "\n",
      "    accuracy                           0.75      4239\n",
      "   macro avg       0.76      0.74      0.75      4239\n",
      "weighted avg       0.75      0.75      0.75      4239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_loss, val_acc, val_pred, val_true = eval_epoch_bert(val_loader, model, criterion, device)\n",
    "\n",
    "val_results = evaluate_classification(val_true, val_pred)\n",
    "print(\"Validation Results:\")\n",
    "print(f\"Accuracy: {val_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {val_results['f1_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(val_results['report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.7580\n",
      "F1 Macro: 0.7512\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78      1604\n",
      "           1       0.76      0.67      0.72       876\n",
      "           2       0.74      0.78      0.76      1760\n",
      "\n",
      "    accuracy                           0.76      4240\n",
      "   macro avg       0.76      0.74      0.75      4240\n",
      "weighted avg       0.76      0.76      0.76      4240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_pred, test_true = eval_epoch_bert(test_loader, model, criterion, device)\n",
    "\n",
    "test_results = evaluate_classification(test_true, test_pred)\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {test_results['f1_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../src/model/bert_finetuned.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "os.makedirs('../src/model', exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_name': model_name,\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'num_classes': 3\n",
    "}, '../src/model/bert_finetuned.pt')\n",
    "\n",
    "print(\"Model saved to ../src/model/bert_finetuned.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_tickets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
