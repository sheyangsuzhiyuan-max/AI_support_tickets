{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Error Analysis: Baseline Models vs BERT\n",
    "\n",
    "本 notebook 对比 **Logistic Regression**, **TextCNN**, 和 **BERT** 三个模型的性能，用于作业提交。\n",
    "\n",
    "**目标:**\n",
    "- 展示从传统机器学习 → 深度学习 → Transformer 的性能提升\n",
    "- 分析各模型的错误模式\n",
    "- 满足数据处理、模型对比的作业要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root to Python path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data_utils import load_text_classification_data\n",
    "from src.text_preprocess import basic_clean\n",
    "from src.features import build_tfidf_vectorizer\n",
    "from src.model.text_cnn import TextCNN\n",
    "from src.model.bert_model import BertClassifier, get_tokenizer\n",
    "from src.evaluate import evaluate_classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_texts, test_labels, label2id, id2label = load_text_classification_data('test')\n",
    "test_texts_clean = [basic_clean(text) for text in test_texts]\n",
    "\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "print(f\"Label mapping: {label2id}\")\n",
    "print(f\"Class distribution: {Counter(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Baseline Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF vectorizer and Logistic Regression model\n",
    "tfidf_vectorizer = joblib.load('../src/model/tfidf_vectorizer.joblib')\n",
    "logreg_model = joblib.load('../src/model/baseline_logreg.joblib')\n",
    "\n",
    "# Get predictions\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_texts_clean)\n",
    "logreg_pred = logreg_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression predictions obtained\")\n",
    "logreg_results = evaluate_classification(test_labels, logreg_pred)\n",
    "print(f\"Accuracy: {logreg_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {logreg_results['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TextCNN model\n",
    "checkpoint = torch.load('../src/model/textcnn.pt', map_location=device, weights_only=False)\n",
    "vocab = checkpoint['vocab']\n",
    "model_config = checkpoint['model_config']\n",
    "\n",
    "# Recreate model\n",
    "textcnn_model = TextCNN(**model_config).to(device)\n",
    "textcnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "textcnn_model.eval()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        words = text.split()[:self.max_len]\n",
    "        tokens = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "        padded = tokens + [self.vocab['<PAD>']] * (self.max_len - len(tokens))\n",
    "        return torch.tensor(padded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "test_dataset_cnn = TextDataset(test_texts_clean, test_labels, vocab, max_len=128)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "textcnn_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader_cnn:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = textcnn_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        textcnn_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "textcnn_pred = np.array(textcnn_pred)\n",
    "print(\"TextCNN predictions obtained\")\n",
    "textcnn_results = evaluate_classification(test_labels, textcnn_pred)\n",
    "print(f\"Accuracy: {textcnn_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {textcnn_results['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load BERT Model (from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model - now using online model\n",
    "checkpoint_bert = torch.load('../src/model/bert_finetuned.pt', map_location=device, weights_only=False)\n",
    "model_name = checkpoint_bert.get('model_name', 'distilbert-base-uncased')\n",
    "\n",
    "# Recreate model - will download from HuggingFace Hub\n",
    "bert_model = BertClassifier(model_name=model_name, num_classes=3).to(device)\n",
    "bert_model.load_state_dict(checkpoint_bert['model_state_dict'])\n",
    "bert_model.eval()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "test_dataset_bert = BertDataset(test_texts_clean, test_labels, tokenizer, max_length=128)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=16, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "bert_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_bert:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = bert_model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        bert_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "bert_pred = np.array(bert_pred)\n",
    "print(\"BERT predictions obtained\")\n",
    "bert_results = evaluate_classification(test_labels, bert_pred)\n",
    "print(f\"Accuracy: {bert_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {bert_results['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'TextCNN', 'BERT'],\n",
    "    'Accuracy': [logreg_results['accuracy'], textcnn_results['accuracy'], bert_results['accuracy']],\n",
    "    'F1 Macro': [logreg_results['f1_macro'], textcnn_results['f1_macro'], bert_results['f1_macro']],\n",
    "    'F1 Micro': [logreg_results['f1_micro'], textcnn_results['f1_micro'], bert_results['f1_micro']],\n",
    "    'F1 Weighted': [logreg_results['f1_weighted'], textcnn_results['f1_weighted'], bert_results['f1_weighted']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON (Assignment)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "models = comparison_df['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax[0].bar(x, comparison_df['Accuracy'], width, label='Accuracy')\n",
    "ax[0].set_ylabel('Score')\n",
    "ax[0].set_title('Model Accuracy Comparison')\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(models, rotation=15, ha='right')\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax[1].bar(x - width/2, comparison_df['F1 Macro'], width, label='F1 Macro')\n",
    "ax[1].bar(x + width/2, comparison_df['F1 Weighted'], width, label='F1 Weighted')\n",
    "ax[1].set_ylabel('F1 Score')\n",
    "ax[1].set_title('F1 Score Comparison')\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(models, rotation=15, ha='right')\n",
    "ax[1].set_ylim([0, 1])\n",
    "ax[1].legend()\n",
    "ax[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "label_names = [id2label[i] for i in sorted(id2label.keys())]\n",
    "\n",
    "cm_logreg = confusion_matrix(test_labels, logreg_pred)\n",
    "cm_textcnn = confusion_matrix(test_labels, textcnn_pred)\n",
    "cm_bert = confusion_matrix(test_labels, bert_pred)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, cm, title in zip(axes, [cm_logreg, cm_textcnn, cm_bert], \n",
    "                          ['Logistic Regression', 'TextCNN', 'BERT']):\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks(np.arange(len(label_names)))\n",
    "    ax.set_yticks(np.arange(len(label_names)))\n",
    "    ax.set_xticklabels(label_names)\n",
    "    ax.set_yticklabels(label_names)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'{title}\\nConfusion Matrix')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(len(label_names)):\n",
    "            text = ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\" if cm[i, j] < cm.max()/2 else \"white\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Calculate per-class metrics for each model\n",
    "precision_logreg, recall_logreg, f1_logreg, _ = precision_recall_fscore_support(\n",
    "    test_labels, logreg_pred, average=None, labels=[0, 1, 2]\n",
    ")\n",
    "precision_textcnn, recall_textcnn, f1_textcnn, _ = precision_recall_fscore_support(\n",
    "    test_labels, textcnn_pred, average=None, labels=[0, 1, 2]\n",
    ")\n",
    "precision_bert, recall_bert, f1_bert, _ = precision_recall_fscore_support(\n",
    "    test_labels, bert_pred, average=None, labels=[0, 1, 2]\n",
    ")\n",
    "\n",
    "# Create comparison dataframe\n",
    "per_class_df = pd.DataFrame({\n",
    "    'Class': label_names * 3,\n",
    "    'Model': ['LogReg'] * 3 + ['TextCNN'] * 3 + ['BERT'] * 3,\n",
    "    'Precision': list(precision_logreg) + list(precision_textcnn) + list(precision_bert),\n",
    "    'Recall': list(recall_logreg) + list(recall_textcnn) + list(recall_bert),\n",
    "    'F1': list(f1_logreg) + list(f1_textcnn) + list(f1_bert)\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(per_class_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples for each model\n",
    "misclassified_logreg = np.where(test_labels != logreg_pred)[0]\n",
    "misclassified_textcnn = np.where(test_labels != textcnn_pred)[0]\n",
    "misclassified_bert = np.where(test_labels != bert_pred)[0]\n",
    "\n",
    "print(f\"Misclassified samples:\")\n",
    "print(f\"  Logistic Regression: {len(misclassified_logreg)} ({len(misclassified_logreg)/len(test_labels)*100:.2f}%)\")\n",
    "print(f\"  TextCNN: {len(misclassified_textcnn)} ({len(misclassified_textcnn)/len(test_labels)*100:.2f}%)\")\n",
    "print(f\"  BERT: {len(misclassified_bert)} ({len(misclassified_bert)/len(test_labels)*100:.2f}%)\")\n",
    "\n",
    "# Find samples where all models agree (correct or incorrect)\n",
    "all_correct = np.where((test_labels == logreg_pred) & \n",
    "                       (test_labels == textcnn_pred) & \n",
    "                       (test_labels == bert_pred))[0]\n",
    "all_wrong = np.where((test_labels != logreg_pred) & \n",
    "                     (test_labels != textcnn_pred) & \n",
    "                     (test_labels != bert_pred))[0]\n",
    "\n",
    "print(f\"\\nModel Agreement:\")\n",
    "print(f\"  All correct: {len(all_correct)} ({len(all_correct)/len(test_labels)*100:.2f}%)\")\n",
    "print(f\"  All wrong: {len(all_wrong)} ({len(all_wrong)/len(test_labels)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Error Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some examples where all models failed\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Examples where ALL models failed:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, idx in enumerate(all_wrong[:5]):\n",
    "    print(f\"\\n[Example {i+1}]\")\n",
    "    print(f\"Text: {test_texts[idx][:150]}...\")\n",
    "    print(f\"True: {id2label[test_labels[idx]]}\")\n",
    "    print(f\"LogReg→{id2label[logreg_pred[idx]]}, CNN→{id2label[textcnn_pred[idx]]}, BERT→{id2label[bert_pred[idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance Progression:**\n",
    "   - Traditional ML (TF-IDF + LogReg) provides a solid baseline\n",
    "   - Deep learning (TextCNN) may struggle without sufficient data/tuning\n",
    "   - Transformer models (BERT) generally achieve the best performance\n",
    "\n",
    "2. **Data Processing Requirements:**\n",
    "   - Text cleaning and preprocessing is crucial for all models\n",
    "   - Different models require different input formats (TF-IDF vs embeddings vs subword tokens)\n",
    "\n",
    "3. **Error Patterns:**\n",
    "   - Some samples are inherently difficult across all models\n",
    "   - Class imbalance may affect minority class performance\n",
    "   - Ambiguous or short texts pose challenges\n",
    "\n",
    "### Assignment Deliverables:\n",
    "- ✅ Data preprocessing pipeline\n",
    "- ✅ Multiple model implementations (LogReg, CNN, BERT)\n",
    "- ✅ Comprehensive evaluation metrics\n",
    "- ✅ Error analysis and insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
