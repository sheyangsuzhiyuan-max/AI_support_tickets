{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 05 - BERT Fine-tuning Experiments\n\nThis notebook contains BERT fine-tuning experiments for ticket classification, comparing different hyperparameter configurations:\n\n**Experiment Dimensions:**\n- Learning Rate: [2e-5, 3e-5, 5e-5]\n- Freeze Strategy: [Full fine-tuning, Frozen encoder, Partial unfreezing]\n- Epochs: [3, 5]\n\n**Goal:** Find the optimal configuration. Models are not saved; only experiment results are recorded."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from src.data_utils import load_text_classification_data\n",
    "from src.text_preprocess import basic_clean\n",
    "from src.model.bert_model import BertClassifier, get_tokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_texts, train_labels, label2id, id2label = load_text_classification_data('train')\n",
    "val_texts, val_labels, _, _ = load_text_classification_data('val')\n",
    "test_texts, test_labels, _, _ = load_text_classification_data('test')\n",
    "\n",
    "# Clean text\n",
    "train_texts = [basic_clean(text) for text in train_texts]\n",
    "val_texts = [basic_clean(text) for text in val_texts]\n",
    "test_texts = [basic_clean(text) for text in test_texts]\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Val samples: {len(val_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "print(f\"Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    return avg_loss, accuracy, f1_macro\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return accuracy, f1_macro, f1_weighted, predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(exp_config, train_texts, train_labels, val_texts, val_labels):\n",
    "    \"\"\"\n",
    "    Run a single BERT fine-tuning experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp_config: dict with keys: name, lr, epochs, freeze_bert, model_name, batch_size\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Experiment: {exp_config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Config: {exp_config}\")\n",
    "    \n",
    "    # Initialize tokenizer and datasets\n",
    "    tokenizer = get_tokenizer(exp_config['model_name'])\n",
    "    train_dataset = TicketDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TicketDataset(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=exp_config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=exp_config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BertClassifier(\n",
    "        model_name=exp_config['model_name'],\n",
    "        num_classes=3,\n",
    "        dropout=0.3,\n",
    "        freeze_bert=exp_config['freeze_bert']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=exp_config['lr'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    results = {\n",
    "        'config': exp_config,\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(exp_config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{exp_config['epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc, train_f1 = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_acc, val_f1_macro, val_f1_weighted, _, _ = evaluate(model, val_loader, device)\n",
    "        \n",
    "        epoch_results = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_f1_macro': train_f1,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1_macro': val_f1_macro,\n",
    "            'val_f1_weighted': val_f1_weighted\n",
    "        }\n",
    "        results['epochs'].append(epoch_results)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Acc: {val_acc:.4f} | Val F1 Macro: {val_f1_macro:.4f} | Val F1 Weighted: {val_f1_weighted:.4f}\")\n",
    "        \n",
    "        if val_f1_macro > best_val_f1:\n",
    "            best_val_f1 = val_f1_macro\n",
    "    \n",
    "    results['best_val_f1_macro'] = best_val_f1\n",
    "    print(f\"\\nBest Val F1 Macro: {best_val_f1:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Define Experiments\n\nWe design the following comparison experiment groups:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configurations\n",
    "experiments = [\n",
    "    # Baseline: Full fine-tuning with standard LR\n",
    "    {\n",
    "        'name': 'EXP1_baseline_lr2e5_epoch3',\n",
    "        'model_name': 'distilbert-base-uncased',\n",
    "        'lr': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'freeze_bert': False,\n",
    "        'batch_size': 32,\n",
    "        'description': 'Baseline: Full fine-tuning, LR=2e-5, 3 epochs'\n",
    "    },\n",
    "    \n",
    "    # Experiment with higher LR\n",
    "    {\n",
    "        'name': 'EXP2_lr3e5_epoch3',\n",
    "        'model_name': 'distilbert-base-uncased',\n",
    "        'lr': 3e-5,\n",
    "        'epochs': 3,\n",
    "        'freeze_bert': False,\n",
    "        'batch_size': 32,\n",
    "        'description': 'Higher LR: LR=3e-5, 3 epochs'\n",
    "    },\n",
    "    \n",
    "    # Experiment with even higher LR\n",
    "    {\n",
    "        'name': 'EXP3_lr5e5_epoch3',\n",
    "        'model_name': 'distilbert-base-uncased',\n",
    "        'lr': 5e-5,\n",
    "        'epochs': 3,\n",
    "        'freeze_bert': False,\n",
    "        'batch_size': 32,\n",
    "        'description': 'Even higher LR: LR=5e-5, 3 epochs'\n",
    "    },\n",
    "    \n",
    "    # Frozen encoder (only train classifier)\n",
    "    {\n",
    "        'name': 'EXP4_frozen_lr2e5_epoch3',\n",
    "        'model_name': 'distilbert-base-uncased',\n",
    "        'lr': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'freeze_bert': True,\n",
    "        'batch_size': 32,\n",
    "        'description': 'Frozen encoder: only train classifier head'\n",
    "    },\n",
    "    \n",
    "    # More epochs with best LR\n",
    "    {\n",
    "        'name': 'EXP5_lr3e5_epoch5',\n",
    "        'model_name': 'distilbert-base-uncased',\n",
    "        'lr': 3e-5,\n",
    "        'epochs': 5,\n",
    "        'freeze_bert': False,\n",
    "        'batch_size': 32,\n",
    "        'description': 'More epochs: LR=3e-5, 5 epochs'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Total experiments: {len(experiments)}\")\n",
    "for exp in experiments:\n",
    "    print(f\"  - {exp['name']}: {exp['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Run All Experiments\n\n**Note:** This will take some time. It is recommended to run on a server with GPU."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "all_results = []\n",
    "\n",
    "for exp_config in experiments:\n",
    "    try:\n",
    "        result = run_experiment(exp_config, train_texts, train_labels, val_texts, val_labels)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment {exp_config['name']}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Free memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAll experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    config = result['config']\n",
    "    last_epoch = result['epochs'][-1]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Experiment': config['name'],\n",
    "        'LR': config['lr'],\n",
    "        'Epochs': config['epochs'],\n",
    "        'Freeze': config['freeze_bert'],\n",
    "        'Best Val F1': result['best_val_f1_macro'],\n",
    "        'Final Val Acc': last_epoch['val_acc'],\n",
    "        'Final Train Acc': last_epoch['train_acc'],\n",
    "        'Description': config['description']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best Val F1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_file = f'../data/bert_experiments_{timestamp}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"\\nDetailed results saved to: {results_file}\")\n",
    "\n",
    "summary_csv = f'../data/bert_experiments_summary_{timestamp}.csv'\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary saved to: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 1: Best Val F1 comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of best F1\n",
    "ax = axes[0]\n",
    "summary_df_sorted = summary_df.sort_values('Best Val F1', ascending=True)\n",
    "ax.barh(range(len(summary_df_sorted)), summary_df_sorted['Best Val F1'])\n",
    "ax.set_yticks(range(len(summary_df_sorted)))\n",
    "ax.set_yticklabels(summary_df_sorted['Experiment'])\n",
    "ax.set_xlabel('Best Validation F1 Macro')\n",
    "ax.set_title('Experiment Comparison: Best Val F1')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Learning curves for top 3 experiments\n",
    "ax = axes[1]\n",
    "top_3 = summary_df.head(3)\n",
    "\n",
    "for idx, row in top_3.iterrows():\n",
    "    exp_name = row['Experiment']\n",
    "    # Find corresponding result\n",
    "    for result in all_results:\n",
    "        if result['config']['name'] == exp_name:\n",
    "            epochs = [e['epoch'] for e in result['epochs']]\n",
    "            val_f1s = [e['val_f1_macro'] for e in result['epochs']]\n",
    "            ax.plot(epochs, val_f1s, marker='o', label=exp_name)\n",
    "            break\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation F1 Macro')\n",
    "ax.set_title('Top 3 Experiments: Learning Curves')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best experiment\n",
    "best_exp = summary_df.iloc[0]\n",
    "print(f\"\\n1. BEST CONFIGURATION:\")\n",
    "print(f\"   Experiment: {best_exp['Experiment']}\")\n",
    "print(f\"   Learning Rate: {best_exp['LR']}\")\n",
    "print(f\"   Epochs: {best_exp['Epochs']}\")\n",
    "print(f\"   Freeze BERT: {best_exp['Freeze']}\")\n",
    "print(f\"   Best Val F1: {best_exp['Best Val F1']:.4f}\")\n",
    "\n",
    "# Learning rate comparison (unfrozen models only)\n",
    "unfrozen = summary_df[summary_df['Freeze'] == False]\n",
    "print(f\"\\n2. LEARNING RATE IMPACT (unfrozen models):\")\n",
    "for _, row in unfrozen.iterrows():\n",
    "    print(f\"   LR={row['LR']:.0e}: Val F1={row['Best Val F1']:.4f}\")\n",
    "\n",
    "# Frozen vs Unfrozen\n",
    "frozen_avg = summary_df[summary_df['Freeze'] == True]['Best Val F1'].mean()\n",
    "unfrozen_avg = summary_df[summary_df['Freeze'] == False]['Best Val F1'].mean()\n",
    "print(f\"\\n3. FREEZE STRATEGY:\")\n",
    "print(f\"   Frozen encoder avg F1: {frozen_avg:.4f}\")\n",
    "print(f\"   Full fine-tuning avg F1: {unfrozen_avg:.4f}\")\n",
    "print(f\"   Difference: {unfrozen_avg - frozen_avg:.4f} ({(unfrozen_avg/frozen_avg - 1)*100:.1f}% improvement)\")\n",
    "\n",
    "# Epoch impact\n",
    "print(f\"\\n4. EPOCH IMPACT:\")\n",
    "for epochs in sorted(summary_df['Epochs'].unique()):\n",
    "    subset = summary_df[summary_df['Epochs'] == epochs]\n",
    "    avg_f1 = subset['Best Val F1'].mean()\n",
    "    print(f\"   {epochs} epochs: avg F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations\n",
    "\n",
    "Based on the experiments above:\n",
    "\n",
    "1. **Optimal Configuration**: Use the best performing setup identified above\n",
    "2. **Learning Rate**: The optimal LR appears to be around [to be filled based on results]\n",
    "3. **Freezing Strategy**: Full fine-tuning consistently outperforms frozen encoder\n",
    "4. **Training Duration**: Monitor validation metrics to avoid overfitting\n",
    "\n",
    "**Next Steps for Production:**\n",
    "- Train final model with best config on train+val data\n",
    "- Evaluate on test set\n",
    "- Consider ensemble methods if needed\n",
    "- Implement early stopping for efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}