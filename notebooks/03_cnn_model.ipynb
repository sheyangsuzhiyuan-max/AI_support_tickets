{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN Model\n",
    "\n",
    "This notebook implements a clean TextCNN model for text classification using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root to Python path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from src.data_utils import load_text_classification_data\n",
    "from src.text_preprocess import basic_clean\n",
    "from src.model.text_cnn import TextCNN\n",
    "from src.train_nn import train_epoch, eval_epoch\n",
    "from src.evaluate import evaluate_classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 19782\n",
      "Validation samples: 4239\n",
      "Test samples: 4240\n",
      "Label mapping: {'high': 0, 'low': 1, 'medium': 2}\n",
      "Label distribution (train): [7698 4043 8041]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_texts, train_labels, label2id, id2label = load_text_classification_data('train')\n",
    "val_texts, val_labels, _, _ = load_text_classification_data('val')\n",
    "test_texts, test_labels, _, _ = load_text_classification_data('test')\n",
    "\n",
    "# Clean text\n",
    "train_texts_clean = [basic_clean(text) for text in train_texts]\n",
    "val_texts_clean = [basic_clean(text) for text in val_texts]\n",
    "test_texts_clean = [basic_clean(text) for text in test_texts]\n",
    "\n",
    "print(f\"Training samples: {len(train_texts_clean)}\")\n",
    "print(f\"Validation samples: {len(val_texts_clean)}\")\n",
    "print(f\"Test samples: {len(test_texts_clean)}\")\n",
    "print(f\"Label mapping: {label2id}\")\n",
    "print(f\"Label distribution (train): {np.bincount(train_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Vocabulary & Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7289\n",
      "Special tokens: <PAD>=0, <UNK>=1\n",
      "Sample vocab items: [('<PAD>', 0), ('<UNK>', 1), ('enhance', 2), ('investment', 3), ('strategy', 4), ('with', 5), ('machine', 6), ('learning', 7), ('hello', 8), ('customer', 9)]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(texts, min_freq=1):\n",
    "    \"\"\"\n",
    "    Build vocabulary from texts using whitespace tokenization.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        min_freq: Minimum frequency threshold (default: 1, includes all tokens)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping tokens to IDs\n",
    "        Special tokens: <PAD>=0, <UNK>=1\n",
    "    \"\"\"\n",
    "    # Count all tokens using whitespace tokenization\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        # Whitespace tokenization: text.split()\n",
    "        words = text.split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Create vocabulary with special tokens\n",
    "    # <PAD> = 0, <UNK> = 1\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = 2\n",
    "    \n",
    "    # Add all tokens with frequency >= min_freq\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary with min_freq=1 (include all tokens)\n",
    "vocab = build_vocab(train_texts_clean, min_freq=1)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Special tokens: <PAD>={vocab['<PAD>']}, <UNK>={vocab['<UNK>']}\")\n",
    "print(f\"Sample vocab items: {list(vocab.items())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for text classification.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        labels: List or array of label IDs\n",
    "        vocab: Dictionary mapping tokens to IDs\n",
    "        max_len: Maximum sequence length (default: 256)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (padded_tensor, label)\n",
    "            - padded_tensor: Tensor of shape (max_len,) with token IDs\n",
    "            - label: Tensor with label ID\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize using whitespace: text.split()\n",
    "        words = text.split()\n",
    "        \n",
    "        # Truncate to max_len\n",
    "        words = words[:self.max_len]\n",
    "        \n",
    "        # Convert tokens to IDs\n",
    "        # Use <UNK> (ID=1) for tokens not in vocabulary\n",
    "        tokens = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "        \n",
    "        # Pad sequence to max_len using <PAD> (ID=0)\n",
    "        padded = tokens + [self.vocab['<PAD>']] * (self.max_len - len(tokens))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        padded_tensor = torch.tensor(padded, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return padded_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 310\n",
      "Val batches: 67\n",
      "Test batches: 67\n",
      "\n",
      "Sample tensor shape: torch.Size([256])\n",
      "Sample label: 0\n",
      "Sample tensor (first 10): tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "max_len = 256\n",
    "train_dataset = TextDataset(train_texts_clean, train_labels, vocab, max_len=max_len)\n",
    "val_dataset = TextDataset(val_texts_clean, val_labels, vocab, max_len=max_len)\n",
    "test_dataset = TextDataset(test_texts_clean, test_labels, vocab, max_len=max_len)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Verify dataset returns correct format\n",
    "sample_tensor, sample_label = train_dataset[0]\n",
    "print(f\"\\nSample tensor shape: {sample_tensor.shape}\")\n",
    "print(f\"Sample label: {sample_label}\")\n",
    "print(f\"Sample tensor (first 10): {sample_tensor[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,087,795\n",
      "\n",
      "Model architecture:\n",
      "TextCNN(\n",
      "  (embedding): Embedding(7289, 128, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(128, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(128, 100, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(128, 100, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize TextCNN model\n",
    "embed_dim = 128\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_classes = 3\n",
    "dropout = 0.5\n",
    "padding_idx = 0\n",
    "\n",
    "model = TextCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_filters=num_filters,\n",
    "    filter_sizes=filter_sizes,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout,\n",
    "    padding_idx=padding_idx,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.8566, 1.6310, 0.8200], device='cuda:0')\n",
      "Class distribution: [7698 4043 8041]\n",
      "\n",
      "Training for 5 epochs...\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights to handle class imbalance\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"Class distribution: {np.bincount(train_labels)}\")\n",
    "\n",
    "# Setup optimizer and loss with class weights\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "num_epochs = 5\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 1.1592, Train Acc: 0.3800\n",
      "  Val Loss: 1.0463, Val Acc: 0.4999\n",
      "\n",
      "Epoch 2/5\n",
      "  Train Loss: 1.0465, Train Acc: 0.4499\n",
      "  Val Loss: 1.0312, Val Acc: 0.5339\n",
      "\n",
      "Epoch 3/5\n",
      "  Train Loss: 0.9935, Train Acc: 0.5000\n",
      "  Val Loss: 0.9560, Val Acc: 0.5041\n",
      "\n",
      "Epoch 4/5\n",
      "  Train Loss: 0.9244, Train Acc: 0.5493\n",
      "  Val Loss: 0.9054, Val Acc: 0.5607\n",
      "\n",
      "Epoch 5/5\n",
      "  Train Loss: 0.8262, Train Acc: 0.6160\n",
      "  Val Loss: 0.8648, Val Acc: 0.6110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(train_loader, model, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = eval_epoch(val_loader, model, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "Accuracy: 0.6110\n",
      "F1 Macro: 0.6024\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      1615\n",
      "           1       0.53      0.62      0.57       855\n",
      "           2       0.69      0.48      0.57      1769\n",
      "\n",
      "    accuracy                           0.61      4239\n",
      "   macro avg       0.61      0.62      0.60      4239\n",
      "weighted avg       0.62      0.61      0.61      4239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set (use unweighted loss for evaluation)\n",
    "eval_criterion = nn.CrossEntropyLoss()\n",
    "val_loss, val_acc, val_pred, val_true = eval_epoch(val_loader, model, eval_criterion, device)\n",
    "\n",
    "val_results = evaluate_classification(val_true, val_pred)\n",
    "print(\"Validation Results:\")\n",
    "print(f\"Accuracy: {val_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {val_results['f1_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(val_results['report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.6035\n",
      "F1 Macro: 0.5930\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      1604\n",
      "           1       0.50      0.62      0.55       876\n",
      "           2       0.70      0.47      0.56      1760\n",
      "\n",
      "    accuracy                           0.60      4240\n",
      "   macro avg       0.60      0.61      0.59      4240\n",
      "weighted avg       0.62      0.60      0.60      4240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set (use unweighted loss for evaluation)\n",
    "test_loss, test_acc, test_pred, test_true = eval_epoch(test_loader, model, eval_criterion, device)\n",
    "\n",
    "test_results = evaluate_classification(test_true, test_pred)\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro: {test_results['f1_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../src/model/textcnn.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "os.makedirs('../src/model', exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'model_config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_dim': embed_dim,\n",
    "        'num_filters': num_filters,\n",
    "        'filter_sizes': filter_sizes,\n",
    "        'num_classes': num_classes,\n",
    "        'dropout': dropout,\n",
    "        'padding_idx': padding_idx\n",
    "    }\n",
    "}, '../src/model/textcnn.pt')\n",
    "\n",
    "print(\"Model saved to ../src/model/textcnn.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
