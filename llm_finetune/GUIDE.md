# æ“ä½œæŒ‡å—

## ğŸ“ ç›®å½•ç»“æ„è¯´æ˜

æœ¬é¡¹ç›®ä½¿ç”¨ä»¥ä¸‹ç›®å½•ç»“æ„ï¼ˆè„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰ï¼š

```
/mnt/kai_ckp/alex/                      # ä½ çš„å·¥ä½œç›®å½•
â”œâ”€â”€ 000_ai_support_tickets/             # Git å…‹éš†çš„é¡¹ç›®
â”‚   â”œâ”€â”€ llm_finetune/                   # å¾®è°ƒå­é¡¹ç›®
â”‚   â””â”€â”€ data/                           # åŸå§‹æ•°æ®
â”œâ”€â”€ models/                             # æ¨¡å‹ç›®å½•
â”‚   â””â”€â”€ qwen/Qwen2-7B-Instruct/
â””â”€â”€ LLaMA-Factory/                      # è®­ç»ƒæ¡†æ¶
    â””â”€â”€ outputs/                        # è®­ç»ƒè¾“å‡º
```

**æ³¨æ„**ï¼šæ–‡æ¡£ä¸­çš„ç¤ºä¾‹è·¯å¾„åŸºäºä¸Šè¿°ç»“æ„ï¼Œå®é™…ä½¿ç”¨æ—¶è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹ä½ çš„å·¥ä½œç›®å½•ã€‚

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### LoRA Rank å¯¹æ¯”å®éªŒï¼ˆ3ä¸ªï¼‰

ä½¿ç”¨ç»éªŒå€¼è¶…å‚ï¼Œåªå¯¹æ¯”ä¸åŒ rank çš„æ•ˆæœï¼š

```bash
# SSH ç™»å½•æœåŠ¡å™¨
ssh username@server
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune

# å‰å°è¿è¡Œï¼ˆå¯ä»¥çœ‹åˆ°å®æ—¶è¾“å‡ºï¼‰
bash scripts/run_rank_comparison.sh

# å¦‚æœæƒ³æ¨åå°å¹¶æ–­å¼€SSHï¼š
# 1. æŒ‰ Ctrl+Z æš‚åœ
# 2. è¾“å…¥ bg è®©å®ƒåå°ç»§ç»­è¿è¡Œ
# 3. è¾“å…¥ disown è„±ç¦»ç»ˆç«¯
# 4. å¯ä»¥å®‰å…¨é€€å‡º SSH

# æŸ¥çœ‹æ—¥å¿—ï¼ˆå¦‚æœå·²æ¨åå°ï¼‰
tail -f training.log
```

**3ä¸ªå®éªŒ**ï¼š
- Rank 32: å¿«é€Ÿï¼ˆ~2hï¼‰
- Rank 64: æ¨èï¼Œå¹³è¡¡ï¼ˆ~2.5hï¼‰â­
- Rank 128: æ…¢é€Ÿï¼ˆ~3hï¼‰

**å›ºå®šè¶…å‚**ï¼ˆç»éªŒå€¼ï¼‰ï¼š
- Learning Rate: 2e-4
- Epochs: 3
- Warmup: 0.05
- Batch Size: 4 Ã— 8 = 32

**è¯„ä¼°**ï¼š
```bash
# è®­ç»ƒå®Œæˆåè¯„ä¼°
bash scripts/evaluate_rank_comparison.sh
# ç”ŸæˆæŠ¥å‘Šï¼ševaluation/rank_comparison_report.md
```

---

## ğŸ“‹ å®Œæ•´æ“ä½œæµç¨‹

### æµç¨‹å›¾

```
æœ¬åœ°ï¼šå‡†å¤‡æ•°æ® â†’ Git ä¸Šä¼ 
  â†“
æœåŠ¡å™¨ï¼šé…ç½®ç¯å¢ƒ â†’ ä¸‹è½½æ¨¡å‹ â†’ è®­ç»ƒ â†’ è¯„ä¼°
  â†“
æœ¬åœ°ï¼šä¸‹è½½ç»“æœ â†’ åˆ†ææŠ¥å‘Š
```

---

## ç¬¬ä¸€æ­¥ï¼šæœ¬åœ°å‡†å¤‡æ•°æ®ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
cd llm_finetune/scripts
python prepare_data.py --task_type multi_task
```

**è¾“å‡º**ï¼š
- `data/alpaca_multi_task_train.json` (19,782 æ¡)
- `data/alpaca_multi_task_val.json` (4,239 æ¡)
- `data/alpaca_multi_task_test.json` (4,240 æ¡)

---

## ç¬¬äºŒæ­¥ï¼šä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼ˆGitï¼‰

```bash
# æœ¬åœ°ï¼šåˆå§‹åŒ– Gitï¼ˆé¦–æ¬¡ï¼‰
git init
git add .
git commit -m "Initial commit: LLM finetune project"
git remote add origin https://your-repo.git
git push -u origin main

# åç»­æ›´æ–°
git add .
git commit -m "Update configs"
git push
```

---

## ç¬¬ä¸‰æ­¥ï¼šæœåŠ¡å™¨é…ç½®ï¼ˆ30åˆ†é’Ÿï¼‰

```bash
# SSH ç™»å½•
ssh username@server

# å…‹éš†æ•´ä¸ªé¡¹ç›®åˆ°ä½ çš„å·¥ä½œç›®å½•
cd /mnt/kai_ckp/alex  # ä½ çš„å·¥ä½œç›®å½•
git clone https://your-repo.git

# è¿›å…¥å¾®è°ƒé¡¹ç›®ç›®å½•
cd 000_ai_support_tickets/llm_finetune

# ä¸€é”®é…ç½®ç¯å¢ƒï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹å·¥ä½œç›®å½•ï¼‰
bash scripts/setup_server.sh
# è¿™ä¼šè‡ªåŠ¨ï¼š
# - å®‰è£… Miniconda
# - åˆ›å»º Python ç¯å¢ƒ
# - å®‰è£… PyTorch + LlamaFactory
# - åœ¨å·¥ä½œç›®å½•åˆ›å»º models/ å’Œ LLaMA-Factory/
```

---

## ç¬¬å››æ­¥ï¼šä¸‹è½½æ¨¡å‹ï¼ˆ20åˆ†é’Ÿï¼‰

```bash
# setup_server.sh ä¼šæç¤ºæ˜¯å¦ä¸‹è½½
# å¦‚æœè·³è¿‡äº†ï¼Œæ‰‹åŠ¨ä¸‹è½½ï¼š
cd /mnt/kai_ckp/alex  # å›åˆ°å·¥ä½œç›®å½•
pip install modelscope

# ä¸‹è½½ Qwen2-7B-Instruct (~14GB)
python -c "
from modelscope import snapshot_download
snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='./models')
"
```

**æ¨¡å‹ä½ç½®**ï¼š`/mnt/kai_ckp/alex/models/qwen/Qwen2-7B-Instruct/`

---

## ç¬¬äº”æ­¥ï¼šå‡†å¤‡è®­ç»ƒï¼ˆ2åˆ†é’Ÿï¼‰

```bash
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune
bash scripts/prepare_training.sh
```

**è¿™ä¸ªè„šæœ¬ä¼š**ï¼š
1. å¤åˆ¶é…ç½®æ–‡ä»¶åˆ° `LLaMA-Factory/configs/`
2. é“¾æ¥æ•°æ®æ–‡ä»¶åˆ° `LLaMA-Factory/data/`
3. æ›´æ–°é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„

---

## ç¬¬å…­æ­¥ï¼šå¼€å§‹è®­ç»ƒ

### æ–¹å¼1: æ‰¹é‡è¿è¡Œ3ä¸ªå®éªŒï¼ˆæ¨èï¼‰

**æ–¹å¼Aï¼šå‰å°è¿è¡Œ**ï¼ˆæ¨èï¼Œå¯çœ‹åˆ°å®æ—¶è¾“å‡ºï¼‰
```bash
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune
bash scripts/run_rank_comparison.sh

# å¦‚æœä¸­é€”æƒ³æ¨åå°ï¼ˆå¯é€‰ï¼‰ï¼š
# 1. æŒ‰ Ctrl+Z æš‚åœ
# 2. è¾“å…¥ bg å›è½¦
# 3. è¾“å…¥ disown å›è½¦
# 4. ç¨‹åºç»§ç»­åœ¨åå°è¿è¡Œï¼Œå¯ä»¥é€€å‡º SSH
```

**æ–¹å¼Bï¼šç›´æ¥åå°è¿è¡Œ**ï¼ˆä¸€å¼€å§‹å°±åå°ï¼‰
```bash
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune
nohup bash scripts/run_rank_comparison.sh > training.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f training.log              # å®æ—¶æŸ¥çœ‹
tail -n 100 training.log          # æŸ¥çœ‹æœ€å100è¡Œ
grep "å®Œæˆ" training.log           # æœç´¢å®ŒæˆçŠ¶æ€

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep run_rank_comparison

# åœæ­¢è®­ç»ƒ
pkill -f run_rank_comparison
```

### æ–¹å¼2: å•ä¸ªå®éªŒ

**å‰å°è¿è¡Œ**ï¼š
```bash
cd /mnt/kai_ckp/alex/LLaMA-Factory
conda activate llm_finetune
llamafactory-cli train configs/qwen2_7b_lora_sft.yaml  # rank64

# éœ€è¦æ—¶æ¨åå°: Ctrl+Z, bg, disown
```

**åå°è¿è¡Œ**ï¼š
```bash
cd /mnt/kai_ckp/alex/LLaMA-Factory
conda activate llm_finetune
nohup llamafactory-cli train configs/qwen2_7b_lora_sft.yaml > training.log 2>&1 &
tail -f training.log
```

### æ–¹å¼3: Web UIï¼ˆå¯è§†åŒ–ï¼‰

```bash
cd /mnt/kai_ckp/alex/LLaMA-Factory
llamafactory-cli webui

# ç«¯å£è½¬å‘ï¼ˆæœ¬åœ°ï¼‰
ssh -L 7860:localhost:7860 username@server

# è®¿é—® http://localhost:7860
```

---

## ç¬¬ä¸ƒæ­¥ï¼šç›‘æ§è®­ç»ƒ

**å‰å°è¿è¡Œæ—¶**ï¼š
```bash
# å®æ—¶è¾“å‡ºä¼šç›´æ¥æ˜¾ç¤ºåœ¨ç»ˆç«¯
# æŒ‰ Ctrl+C å¯ä»¥ä¸­æ–­è®­ç»ƒ

# åœ¨å¦ä¸€ä¸ªSSHçª—å£æŸ¥çœ‹GPU
nvidia-smi
watch -n 1 nvidia-smi  # æ¯ç§’åˆ·æ–°
```

**åå°è¿è¡Œæ—¶**ï¼š
```bash
# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep llamafactory

# æŸ¥çœ‹æ—¥å¿—
tail -f training.log              # å®æ—¶æŸ¥çœ‹
tail -n 100 training.log          # æœ€å100è¡Œ

# æŸ¥çœ‹ GPU
nvidia-smi
```

**ä½¿ç”¨ TensorBoard**ï¼ˆå¯é€‰ï¼‰ï¼š
```bash
# åœ¨æœåŠ¡å™¨ä¸Šå¯åŠ¨ï¼ˆæ–°ç»ˆç«¯æˆ–ç”¨ nohupï¼‰
cd /mnt/kai_ckp/alex/LLaMA-Factory
nohup tensorboard --logdir ./outputs --port 6006 > tensorboard.log 2>&1 &

# æœ¬åœ°è®¿é—®ï¼ˆç«¯å£è½¬å‘ï¼‰
ssh -L 6006:localhost:6006 username@server
# æµè§ˆå™¨æ‰“å¼€: http://localhost:6006
```

---

## ç¬¬å…«æ­¥ï¼šè¯„ä¼°æ¨¡å‹

### æ‰¹é‡è¯„ä¼°ï¼ˆæ¨èï¼‰

```bash
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune
bash scripts/evaluate_rank_comparison.sh
# ç”ŸæˆæŠ¥å‘Šï¼ševaluation/rank_comparison_report.md
```

### å•ä¸ªæ¨¡å‹è¯„ä¼°

```bash
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune

# ç”Ÿæˆé¢„æµ‹
python scripts/inference.py \
    --model_path /mnt/kai_ckp/alex/LLaMA-Factory/outputs/qwen2-7b-ticket-lora \
    --base_model /mnt/kai_ckp/alex/models/qwen/Qwen2-7B-Instruct \
    --use_lora \
    --test_data ./data/alpaca_multi_task_test.json \
    --output ./evaluation/predictions.json

# è¿è¡Œè¯„ä¼°
python scripts/evaluate.py \
    --predictions ./evaluation/predictions.json \
    --references ./data/alpaca_multi_task_test.json \
    --task_type multi_task \
    --output_dir ./evaluation

# æŸ¥çœ‹æŠ¥å‘Š
cat ./evaluation/evaluation_report.txt
```

---

## ç¬¬ä¹æ­¥ï¼šä¸‹è½½ç»“æœ

```bash
# åœ¨æœåŠ¡å™¨ä¸Šå‹ç¼©
cd /mnt/kai_ckp/alex/LLaMA-Factory/outputs
tar -czf qwen2-7b-ticket-lora.tar.gz qwen2-7b-ticket-lora/

# æœ¬åœ°ä¸‹è½½ï¼ˆLoRA æƒé‡ ~300-800 MBï¼‰
scp username@server:/mnt/kai_ckp/alex/LLaMA-Factory/outputs/qwen2-7b-ticket-lora.tar.gz ./models/

# ä¸‹è½½è¯„ä¼°æŠ¥å‘Š
scp username@server:/mnt/kai_ckp/alex/llm_finetune/evaluation/rank_comparison_report.md ./
```

---

## ç¬¬åæ­¥ï¼šæœ¬åœ°æµ‹è¯•ï¼ˆå¯é€‰ï¼‰

```bash
# æœ¬åœ°ä¸‹è½½åŸºç¡€æ¨¡å‹
pip install modelscope
python -c "
from modelscope import snapshot_download
snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='./models')
"

# è§£å‹ LoRA æƒé‡
cd models
tar -xzf qwen2-7b-ticket-lora.tar.gz

# äº¤äº’å¼æ¨ç†
cd ..
python scripts/inference.py \
    --model_path ./models/qwen2-7b-ticket-lora \
    --base_model ./models/qwen/Qwen2-7B-Instruct \
    --use_lora \
    --interactive
```

---

## å¸¸è§é—®é¢˜

### Q: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

```bash
# ä»æ£€æŸ¥ç‚¹æ¢å¤
llamafactory-cli train configs/qwen2_7b_lora_sft.yaml \
    --resume_from_checkpoint ./outputs/qwen2-7b-ticket-lora/checkpoint-500
```

### Q: æ˜¾å­˜ä¸è¶³ï¼Ÿ

```bash
# å‡å° batch size
nano configs/qwen2_7b_lora_sft.yaml
# ä¿®æ”¹ï¼š
# per_device_train_batch_size: 2  # ä» 4 æ”¹ä¸º 2
# gradient_accumulation_steps: 16  # ä» 8 æ”¹ä¸º 16
```

### Q: å¦‚ä½•è°ƒæ•´å‚æ•°ï¼Ÿ

å‚è§ [TUTORIAL.md](TUTORIAL.md) çš„å‚æ•°è°ƒä¼˜éƒ¨åˆ†ã€‚

### Q: å¦‚ä½•å¯¹æ¯”å¤šä¸ªå®éªŒï¼Ÿ

```bash
# ä½¿ç”¨ TensorBoardï¼ˆåå°è¿è¡Œï¼‰
cd /mnt/kai_ckp/alex/LLaMA-Factory
nohup tensorboard --logdir ./outputs --port 6006 > tensorboard.log 2>&1 &
# æœ¬åœ°ç«¯å£è½¬å‘: ssh -L 6006:localhost:6006 username@server

# æˆ–æŸ¥çœ‹è‡ªåŠ¨ç”Ÿæˆçš„å¯¹æ¯”æŠ¥å‘Š
cat ./evaluation/rank_comparison_report.md
```

### Q: å‰å°è¿è¡Œå¦‚ä½•æ¨åˆ°åå°ï¼Ÿ

```bash
# è¿è¡Œä¸­çš„ç¨‹åºï¼š
# 1. æŒ‰ Ctrl+Zï¼ˆæš‚åœï¼‰
# 2. è¾“å…¥ bgï¼ˆåå°ç»§ç»­ï¼‰
# 3. è¾“å…¥ disownï¼ˆè„±ç¦»ç»ˆç«¯ï¼‰

# æŸ¥çœ‹åå°è¿›ç¨‹
jobs                              # å½“å‰ç»ˆç«¯çš„åå°ä»»åŠ¡
ps aux | grep run_rank_comparison # æ‰€æœ‰ç›¸å…³è¿›ç¨‹

# åœæ­¢ç¨‹åº
pkill -f run_rank_comparison
```

---

## å®Œæ•´å‘½ä»¤é€ŸæŸ¥

```bash
# === æœ¬åœ° ===
python prepare_data.py --task_type multi_task
git add . && git commit -m "update" && git push

# === æœåŠ¡å™¨ ===
# é¦–æ¬¡é…ç½®
cd /mnt/kai_ckp/alex
git clone https://your-repo.git
bash scripts/setup_server.sh
bash scripts/prepare_training.sh

# è®­ç»ƒï¼ˆ3ä¸ªå®éªŒï¼‰- å‰å°è¿è¡Œ
cd /mnt/kai_ckp/alex/000_ai_support_tickets/llm_finetune
bash scripts/run_rank_comparison.sh
# éœ€è¦æ¨åå°: Ctrl+Z, bg, disown

# è¯„ä¼°
bash scripts/evaluate_rank_comparison.sh

# === æœ¬åœ° ===
# ä¸‹è½½ç»“æœ
scp username@server:/mnt/kai_ckp/alex/llm_finetune/evaluation/rank_comparison_report.md ./
```

---

## é¡¹ç›®æ–‡ä»¶è¯´æ˜

```
llm_finetune/
â”œâ”€â”€ README.md              # é¡¹ç›®æ¦‚è¿°
â”œâ”€â”€ GUIDE.md              # ğŸ‘ˆ æœ¬æ–‡ä»¶ï¼ˆæ“ä½œæµç¨‹ï¼‰
â”œâ”€â”€ TUTORIAL.md           # LoRA å’Œ LlamaFactory æ•™å­¦
â”‚
â”œâ”€â”€ configs/              # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ qwen2_7b_lora_rank32.yaml
â”‚   â”œâ”€â”€ qwen2_7b_lora_sft.yaml        # rank64ï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ qwen2_7b_lora_rank128.yaml
â”‚   â””â”€â”€ dataset_info.json
â”‚
â”œâ”€â”€ scripts/              # è„šæœ¬
â”‚   â”œâ”€â”€ prepare_data.py                # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ setup_server.sh                # æœåŠ¡å™¨é…ç½®
â”‚   â”œâ”€â”€ prepare_training.sh            # è®­ç»ƒå‡†å¤‡
â”‚   â”œâ”€â”€ run_rank_comparison.sh         # è®­ç»ƒ3ä¸ªå®éªŒ
â”‚   â”œâ”€â”€ evaluate_rank_comparison.sh    # è¯„ä¼°3ä¸ªå®éªŒ
â”‚   â”œâ”€â”€ inference.py                   # æ¨ç†
â”‚   â”œâ”€â”€ evaluate.py                    # è¯„ä¼°
â”‚   â””â”€â”€ generate_report.py             # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
â”‚
â”œâ”€â”€ data/                 # è®­ç»ƒæ•°æ®ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â””â”€â”€ evaluation/           # è¯„ä¼°ç»“æœï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
```
