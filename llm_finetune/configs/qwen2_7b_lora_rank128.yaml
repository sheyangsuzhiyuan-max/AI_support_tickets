### 实验4: LoRA rank=128 (大 rank，接近全量微调)

### Model
model_name_or_path: Qwen/Qwen2-7B-Instruct

### Method
stage: sft
do_train: true
finetuning_type: lora

### LoRA Configuration - 实验4: 大 rank
lora_rank: 128
lora_alpha: 256
lora_dropout: 0.1
lora_target: all

### Dataset Configuration
dataset: ticket_multi_task_train
dataset_dir: ./data
template: qwen
cutoff_len: 2048
max_samples: 20000
overwrite_cache: true
preprocessing_num_workers: 16

### Training Hyperparameters
output_dir: ./outputs/qwen2-7b-ticket-lora-rank128
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.01
max_grad_norm: 1.0

### Precision
bf16: true
tf32: true

### Evaluation
val_size: 0.05
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 200

### Logging & Saving
logging_steps: 10
save_steps: 500
save_total_limit: 3
plot_loss: true
report_to: tensorboard

### Memory Optimization
gradient_checkpointing: true
flash_attn: fa2
