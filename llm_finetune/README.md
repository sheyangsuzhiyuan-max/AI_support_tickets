# æ™ºèƒ½å·¥å•ç³»ç»Ÿ - LLM å¾®è°ƒé¡¹ç›®

## é¡¹ç›®æ¦‚è¿°

åŸºäº **Qwen2-7B + LoRA** å¾®è°ƒï¼Œæ„å»ºæ™ºèƒ½å®¢æœå·¥å•ç³»ç»Ÿï¼Œå®ç°ï¼š
- ğŸ·ï¸ è‡ªåŠ¨åˆ†ç±»ï¼ˆtype/queue/priorityï¼‰
- ğŸ’¬ æ™ºèƒ½å›å¤ç”Ÿæˆ
- âœ… äººå·¥å®¡æ ¸ç•Œé¢
- ğŸ“Š å·¥å•æ•°æ®çœ‹æ¿

---

## å¿«é€Ÿå¼€å§‹

### æœ¬åœ°å‡†å¤‡æ•°æ®

```bash
cd llm_finetune/scripts
python prepare_data.py --task_type multi_task
```

### ä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼ˆGitï¼‰

```bash
git init
git add .
git commit -m "Initial commit"
git push
```

### æœåŠ¡å™¨é…ç½®

```bash
ssh username@server
cd /mnt/kai_ckp/alex
git clone https://your-repo.git
cd 000_ai_support_tickets/llm_finetune

# ä¸€é”®é…ç½®ç¯å¢ƒï¼ˆè‡ªåŠ¨æ£€æµ‹è·¯å¾„ï¼‰
bash scripts/setup_server.sh
bash scripts/prepare_training.sh
```

### å¼€å§‹è®­ç»ƒ

```bash
# æ‰¹é‡è¿è¡Œ3ä¸ªrankå¯¹æ¯”å®éªŒï¼ˆæ¨èï¼‰
bash scripts/run_rank_comparison.sh

# æˆ–å•ä¸ªå®éªŒ
cd /mnt/kai_ckp/alex/LLaMA-Factory
llamafactory-cli train configs/qwen2_7b_lora_sft.yaml
```

### è¯„ä¼°ä¸æŠ¥å‘Š

```bash
# æ‰¹é‡è¯„ä¼°å¹¶è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
bash scripts/evaluate_rank_comparison.sh

# æŸ¥çœ‹æŠ¥å‘Š
cat evaluation/rank_comparison_report.md
```

---

## æ–‡æ¡£ç»“æ„

```
llm_finetune/
â”œâ”€â”€ README.md              # é¡¹ç›®æ¦‚è¿°ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ GUIDE.md              # è¯¦ç»†æ“ä½œæµç¨‹æŒ‡å—
â”œâ”€â”€ TUTORIAL.md           # LoRA å’Œ LlamaFactory æ•™ç¨‹
â”œâ”€â”€ .gitignore            # Git å¿½ç•¥æ–‡ä»¶
â”‚
â”œâ”€â”€ configs/              # è®­ç»ƒé…ç½®ï¼ˆ3ä¸ªrankå¯¹æ¯”å®éªŒï¼‰
â”‚   â”œâ”€â”€ qwen2_7b_lora_rank32.yaml
â”‚   â”œâ”€â”€ qwen2_7b_lora_sft.yaml       # rank64ï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ qwen2_7b_lora_rank128.yaml
â”‚   â””â”€â”€ dataset_info.json
â”‚
â”œâ”€â”€ scripts/              # æ ¸å¿ƒè„šæœ¬
â”‚   â”œâ”€â”€ prepare_data.py                # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ setup_server.sh                # æœåŠ¡å™¨é…ç½®
â”‚   â”œâ”€â”€ prepare_training.sh            # è®­ç»ƒå‡†å¤‡
â”‚   â”œâ”€â”€ run_rank_comparison.sh         # è®­ç»ƒ3ä¸ªå®éªŒ
â”‚   â”œâ”€â”€ evaluate_rank_comparison.sh    # è¯„ä¼°3ä¸ªå®éªŒ
â”‚   â”œâ”€â”€ generate_report.py           # è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
â”‚   â”œâ”€â”€ inference.py                 # æ¨ç†
â”‚   â””â”€â”€ evaluate.py                  # è¯„ä¼°
â”‚
â”œâ”€â”€ data/                 # è®­ç»ƒæ•°æ®ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â”œâ”€â”€ evaluation/           # è¯„ä¼°ç»“æœï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â””â”€â”€ app/                  # Streamlit åº”ç”¨
    â””â”€â”€ main.py
```

---

## æ ¸å¿ƒæ–‡æ¡£

### ğŸ“– å¿…è¯»

1. **[GUIDE.md](GUIDE.md)** - å®Œæ•´æ“ä½œæµç¨‹ + å®éªŒè§„åˆ’
   - ä»é›¶åˆ°ä¸€ï¼šæœ¬åœ°å‡†å¤‡æ•°æ® â†’ æœåŠ¡å™¨é…ç½® â†’ è®­ç»ƒ â†’ è¯„ä¼° â†’ ä¸‹è½½ç»“æœ
   - å®Œæ•´å®éªŒè®¾è®¡ï¼ˆLoRA + è¶…å‚æ•°ï¼Œå…± 8 ç»„å®éªŒï¼‰
   - è¿è¡Œç­–ç•¥ï¼ˆåˆ†é˜¶æ®µ vs ä¸€æ¬¡æ€§ï¼‰
   - å®éªŒè®°å½•è¡¨ä¸é¢„æœŸç»“æœ

2. **[TUTORIAL.md](TUTORIAL.md)** - LoRA å’Œ LlamaFactory æ•™ç¨‹
   - LoRA å‚æ•°è¯¦è§£ï¼ˆrank, alpha, dropout, targetï¼‰
   - è®­ç»ƒè¶…å‚æ•°ï¼ˆlr, epochs, batch sizeï¼‰
   - LlamaFactory ä½¿ç”¨æ–¹æ³•
   - å‚æ•°è°ƒä¼˜å®è·µ

---

## ç¡¬ä»¶é…ç½®

| èµ„æº | é…ç½® |
|------|------|
| GPU | NVIDIA A800 (80GB) |
| æ¨èæ¨¡å‹ | **Qwen2-7B-Instruct** |
| è®­ç»ƒæ˜¾å­˜ | ~24GB (LoRA rank=64) |
| è®­ç»ƒæ—¶é—´ | ~4-6 å°æ—¶ / å®éªŒ |

> ğŸ’¡ A800 80GB å®Œå…¨å¯ä»¥æ”¯æŒ 7B ç”šè‡³ 14B æ¨¡å‹ï¼Œ**1.5B è¿‡äºä¿å®ˆ**

---

## æ¨èé…ç½®

### LoRA å‚æ•°

```yaml
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.1
lora_target: all
```

### è®­ç»ƒå‚æ•°

```yaml
learning_rate: 2.0e-4
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
bf16: true
flash_attn: fa2
```

---

## é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| ROUGE-L F1 | >0.40 | ç”Ÿæˆè´¨é‡ |
| Priority å‡†ç¡®ç‡ | >85% | ä¼˜å…ˆçº§åˆ†ç±» |
| Type å‡†ç¡®ç‡ | >80% | å·¥å•ç±»å‹åˆ†ç±» |
| äººå·¥è¯„ä¼°æ€»åˆ† | >4.0/5.0 | æ•´ä½“æ»¡æ„åº¦ |

---

## å¯¹æ¯”å®éªŒ

### æ¨èï¼šLoRA Rank å¯¹æ¯”ï¼ˆ3ä¸ªå®éªŒï¼‰

**ä½¿ç”¨ç»éªŒå€¼è¶…å‚**ï¼Œåªå¯¹æ¯”ä¸åŒ rank çš„æ•ˆæœï¼š

```bash
# ä¸€é”®è¿è¡Œ3ä¸ªå®éªŒï¼ˆçº¦ 6-7.5 å°æ—¶ï¼‰
bash scripts/run_rank_comparison.sh
```

**å®éªŒé…ç½®**ï¼š
- Rank 32: å¿«é€Ÿï¼ˆ~2hï¼‰
- Rank 64: æ¨èï¼ˆ~2.5hï¼‰â­
- Rank 128: æ…¢é€Ÿï¼ˆ~3hï¼‰

**å›ºå®šè¶…å‚**ï¼ˆç»éªŒå€¼ï¼‰ï¼š
- Learning Rate: 2e-4
- Epochs: 3
- Warmup: 0.05

**è¯„ä¼°**ï¼š
```bash
bash scripts/evaluate_rank_comparison.sh
```

**è¾“å‡º**ï¼š
- `evaluation/rank_comparison_report.md` - LoRA Rank å¯¹æ¯”æŠ¥å‘Š

---

## å¸¸ç”¨å‘½ä»¤

```bash
# æ•°æ®å‡†å¤‡
python scripts/prepare_data.py --task_type multi_task

# è®­ç»ƒ
llamafactory-cli train configs/qwen2_7b_lora_sft.yaml

# Web UI
llamafactory-cli webui

# è¯„ä¼°
python scripts/inference.py --model_path ... --test_data ...
python scripts/evaluate.py --predictions ... --references ...

# æ‰¹é‡å®éªŒ
bash scripts/run_experiments.sh
bash scripts/evaluate_all.sh
```

---

## License

MIT
