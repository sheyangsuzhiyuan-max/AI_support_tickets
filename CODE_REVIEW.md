# ä»£ç å®¡æŸ¥æŠ¥å‘Š

## æ‰§è¡Œæ—¶é—´
2025-12-11

## å®¡æŸ¥èŒƒå›´
- `notebooks/04_BERT_Finetune.ipynb`
- `src/model/bert_model.py`
- `src/train_nn.py`
- `src/data_utils.py`
- `src/evaluate.py`
- `src/text_preprocess.py`

---

## ğŸ”´ ä¸¥é‡é—®é¢˜ï¼ˆå¿…é¡»ä¿®å¤ï¼‰

### 1. Transformersåº“ç¼ºå¤±
**ä½ç½®**: ç¯å¢ƒä¾èµ–

**é—®é¢˜æè¿°**:
```bash
âœ— Transformers import error: No module named 'transformers'
```

**å½±å“**:
- æ— æ³•å¯¼å…¥`BertClassifier`å’Œ`get_tokenizer`
- Notebookç¬¬ä¸€ä¸ªcellå°±ä¼šæŠ¥é”™
- è®­ç»ƒå®Œå…¨æ— æ³•è¿›è¡Œ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: ä½¿ç”¨pipå®‰è£…
pip install transformers

# æ–¹æ³•2: ä»requirements.txtå®‰è£…
pip install -r requirements.txt

# æ–¹æ³•3: ä½¿ç”¨condaï¼ˆå¦‚æœä½¿ç”¨condaç¯å¢ƒï¼‰
conda install -c huggingface transformers
```

**éªŒè¯ä¿®å¤**:
```python
import transformers
print(f"Transformers version: {transformers.__version__}")
# æœŸæœ›è¾“å‡º: Transformers version: 4.x.x
```

---

### 2. CUDA/GPUä¸å¯ç”¨
**ä½ç½®**: è¿è¡Œç¯å¢ƒ

**é—®é¢˜æè¿°**:
```bash
âœ“ CUDA available: False
```

**å½±å“**:
- è®­ç»ƒå°†åœ¨CPUä¸Šè¿è¡Œ
- max_length=512æ—¶ï¼Œå•ä¸ªepochå¯èƒ½éœ€è¦**1-2å°æ—¶**ï¼ˆç›¸æ¯”GPUçš„5-10åˆ†é’Ÿï¼‰
- 10ä¸ªepochå¯èƒ½éœ€è¦**10-20å°æ—¶**

**å½“å‰ç¯å¢ƒ**:
- ç³»ç»Ÿ: macOS (Darwin 24.6.0)
- PyTorch: 2.5.1 (CPUç‰ˆæœ¬)

**è§£å†³æ–¹æ¡ˆé€‰é¡¹**:

**é€‰é¡¹A: ä½¿ç”¨äº‘GPUå¹³å°ï¼ˆæ¨èï¼‰**
```bash
# Google Colab (å…è´¹GPU)
1. è®¿é—® https://colab.research.google.com
2. ä¸Šä¼ notebook
3. è¿è¡Œæ—¶ -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> GPU

# Kaggle (å…è´¹GPU/TPU)
1. è®¿é—® https://www.kaggle.com
2. åˆ›å»ºæ–°notebook
3. è®¾ç½® -> Accelerator -> GPU
```

**é€‰é¡¹B: æœ¬åœ°å®‰è£…CUDA PyTorchï¼ˆå¦‚æœæœ‰NVIDIA GPUï¼‰**
```bash
# å…ˆå¸è½½CPUç‰ˆPyTorch
pip uninstall torch torchvision torchaudio

# å®‰è£…CUDAç‰ˆæœ¬ï¼ˆä»¥CUDA 11.8ä¸ºä¾‹ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# éªŒè¯CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

**é€‰é¡¹C: é™ä½å‚æ•°ç»§ç»­CPUè®­ç»ƒï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰**
```python
# åœ¨notebookä¸­è°ƒæ•´å‚æ•°
max_length = 256  # ä»512é™å›256
batch_size = 8    # ä»16é™åˆ°8
num_epochs = 5    # ä»10é™åˆ°5
```

---

## âš ï¸ é‡è¦é—®é¢˜ï¼ˆå¼ºçƒˆå»ºè®®ä¿®å¤ï¼‰

### 3. ç¼ºå°‘éšæœºç§å­è®¾ç½®
**ä½ç½®**: `notebooks/04_BERT_Finetune.ipynb` - Cell 1

**é—®é¢˜æè¿°**:
- æ²¡æœ‰è®¾ç½®éšæœºç§å­
- æ¯æ¬¡è®­ç»ƒç»“æœä¼šæœ‰éšæœºæ€§
- æ— æ³•å¤ç°å®éªŒç»“æœ

**å½±å“**:
- éš¾ä»¥è°ƒè¯•æ€§èƒ½é—®é¢˜
- è®ºæ–‡/æŠ¥å‘Šä¸­çš„ç»“æœä¸å¯å¤ç°

**è§£å†³æ–¹æ¡ˆ**:
åœ¨Cell 1çš„æœ€åæ·»åŠ ï¼š

```python
# Add after device setup
import random

def set_seed(seed=42):
    """Set random seed for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(42)
print(f"Random seed set to: 42")
```

**é¢„æœŸæ•ˆæœ**:
- ç›¸åŒé…ç½®ä¸‹è®­ç»ƒç»“æœå®Œå…¨ä¸€è‡´
- ä¾¿äºå¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ

---

### 4. æ•°æ®åŠ è½½å¯èƒ½çš„å†…å­˜é—®é¢˜
**ä½ç½®**: `notebooks/04_BERT_Finetune.ipynb` - Cell 9

**æ½œåœ¨é—®é¢˜**:
```python
# BertDatasetä½¿ç”¨ padding='max_length'
# æ‰€æœ‰æ ·æœ¬éƒ½paddingåˆ°512ï¼Œå³ä½¿æ–‡æœ¬å¾ˆçŸ­
```

**å½±å“**:
- çŸ­æ–‡æœ¬ï¼ˆå¦‚50å­—ç¬¦ï¼‰ä¹Ÿä¼šå ç”¨512 tokensçš„å†…å­˜
- DataLoaderå¯èƒ½å ç”¨è¾ƒå¤šå†…å­˜

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼ˆå¯é€‰ï¼‰:
```python
# ä½¿ç”¨dynamic paddingå¯ä»¥èŠ‚çœå†…å­˜
from transformers import DataCollatorWithPadding

# ä¿®æ”¹BertDatasetï¼Œä¸åœ¨__getitem__ä¸­padding
def __getitem__(self, idx):
    text = str(self.texts[idx])
    label = self.labels[idx]

    encoding = self.tokenizer(
        text,
        truncation=True,
        max_length=self.max_length,
        # ç§»é™¤ padding='max_length'
        # ç§»é™¤ return_tensors='pt'
    )

    return {
        'input_ids': encoding['input_ids'],
        'attention_mask': encoding['attention_mask'],
        'labels': label
    }

# ä½¿ç”¨DataCollatoråœ¨batchå±‚é¢åŠ¨æ€padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=data_collator
)
```

**æ³¨æ„**: å½“å‰å®ç°ä¹Ÿæ˜¯æ­£ç¡®çš„ï¼Œè¿™åªæ˜¯æ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼ˆå¯é€‰ï¼‰

### 5. Batch Sizeå¯ä»¥è°ƒæ•´
**ä½ç½®**: `notebooks/04_BERT_Finetune.ipynb` - Cell 9

**å½“å‰é…ç½®**:
```python
batch_size = 16
```

**å»ºè®®**:
- **æœ‰GPUï¼ˆâ‰¥8GBæ˜¾å­˜ï¼‰**: å¢åŠ åˆ°32æˆ–64
- **CPUè®­ç»ƒ**: ä¿æŒ16æˆ–é™åˆ°8
- **äº‘GPUï¼ˆColab/Kaggleï¼‰**: å¯ä»¥å°è¯•32

**å½±å“**:
- æ›´å¤§batch = æ›´ç¨³å®šçš„æ¢¯åº¦ + æ›´å¿«çš„è®­ç»ƒ
- ä½†éœ€è¦æ›´å¤šå†…å­˜

---

### 6. å¯ä»¥æ·»åŠ è®­ç»ƒæ—¥å¿—
**ä½ç½®**: `notebooks/04_BERT_Finetune.ipynb` - Cell 17

**å»ºè®®æ·»åŠ **:
```python
import time

# åœ¨è®­ç»ƒå¾ªç¯å¼€å§‹å‰
start_time = time.time()

# åœ¨æ¯ä¸ªepochç»“æŸå
epoch_time = time.time() - start_time
print(f"  Time: {epoch_time/60:.2f} min")
print(f"  Samples/sec: {len(train_dataset)/epoch_time:.2f}")
start_time = time.time()
```

---

### 7. å¯ä»¥ä¿å­˜è®­ç»ƒå†å²
**ä½ç½®**: `notebooks/04_BERT_Finetune.ipynb` - Cell 17

**å»ºè®®**:
```python
# åœ¨è®­ç»ƒå¾ªç¯å‰
history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': [],
    'learning_rate': []
}

# åœ¨æ¯ä¸ªepochç»“æŸå
history['train_loss'].append(train_loss)
history['train_acc'].append(train_acc)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['learning_rate'].append(scheduler.get_last_lr()[0])

# è®­ç»ƒç»“æŸåå¯è§†åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss Curves')

plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc')
plt.plot(history['val_acc'], label='Val Acc')
plt.legend()
plt.title('Accuracy Curves')
plt.show()
```

---

## âœ… ä»£ç è´¨é‡ä¼˜ç§€çš„éƒ¨åˆ†

### 1. è®­ç»ƒå‡½æ•°è®¾è®¡åˆç†
**æ–‡ä»¶**: `src/train_nn.py`

**ä¼˜ç‚¹**:
- âœ“ æ­£ç¡®ä½¿ç”¨`model.train()`å’Œ`model.eval()`
- âœ“ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ (`clip_grad_norm_`)
- âœ“ Scheduleråœ¨æ¯ä¸ªbatchåstepï¼ˆç¬¦åˆHuggingFaceæœ€ä½³å®è·µï¼‰
- âœ“ è¯„ä¼°æ—¶æ­£ç¡®ä½¿ç”¨`torch.no_grad()`
- âœ“ è¿”å›é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾ç”¨äºåç»­åˆ†æ

### 2. BERTæ¨¡å‹å®ç°æ ‡å‡†
**æ–‡ä»¶**: `src/model/bert_model.py`

**ä¼˜ç‚¹**:
- âœ“ ä½¿ç”¨[CLS] tokençš„hidden stateä½œä¸ºå¥å­è¡¨ç¤ºï¼ˆæ ‡å‡†åšæ³•ï¼‰
- âœ“ Dropoutåº”ç”¨åœ¨åˆ†ç±»å¤´ä¹‹å‰
- âœ“ æ”¯æŒfreeze_bertå‚æ•°ï¼ˆè™½ç„¶å½“å‰æœªä½¿ç”¨ï¼‰
- âœ“ è‡ªåŠ¨ä»configè·å–hidden_size

### 3. æ•°æ®åŠ è½½å¥å£®
**æ–‡ä»¶**: `src/data_utils.py`

**ä¼˜ç‚¹**:
- âœ“ æ­£ç¡®å¤„ç†ç¼ºå¤±å€¼ (`fillna`)
- âœ“ è‡ªåŠ¨åˆ›å»ºlabelæ˜ å°„
- âœ“ è¿”å›label2idå’Œid2labelä¾¿äºåç»­ä½¿ç”¨
- âœ“ è·¯å¾„å¤„ç†ä½¿ç”¨`os.path`ï¼Œè·¨å¹³å°å…¼å®¹

### 4. Early Stoppingå®ç°æ­£ç¡®
**ä½ç½®**: `notebooks/04_BERT_Finetune.ipynb` - Cell 17

**ä¼˜ç‚¹**:
- âœ“ æ­£ç¡®ä¿å­˜æœ€ä½³æ¨¡å‹state_dict
- âœ“ Patienceè®¡æ•°å™¨é€»è¾‘æ­£ç¡®
- âœ“ è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹

### 5. è¯„ä¼°å‡½æ•°å®Œå–„
**æ–‡ä»¶**: `src/evaluate.py`

**ä¼˜ç‚¹**:
- âœ“ è¿”å›å¤šç§F1æŒ‡æ ‡ï¼ˆmacro/micro/weightedï¼‰
- âœ“ åŒ…å«è¯¦ç»†çš„classification report

---

## ğŸ“Š æ•´ä½“ä»£ç è´¨é‡è¯„åˆ†

| ç±»åˆ« | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **ä»£ç ç»“æ„** | â­â­â­â­â­ | æ¨¡å—åŒ–è®¾è®¡ï¼ŒèŒè´£æ¸…æ™° |
| **æœ€ä½³å®è·µ** | â­â­â­â­ | å¤§éƒ¨åˆ†ç¬¦åˆPyTorch/HuggingFaceè§„èŒƒ |
| **é”™è¯¯å¤„ç†** | â­â­â­ | åŸºæœ¬çš„é”™è¯¯æ£€æŸ¥ï¼Œå¯ä»¥æ›´å®Œå–„ |
| **å¯è¯»æ€§** | â­â­â­â­â­ | æ³¨é‡Šæ¸…æ™°ï¼Œå˜é‡å‘½åè§„èŒƒ |
| **å¯ç»´æŠ¤æ€§** | â­â­â­â­ | ä»£ç ç»„ç»‡è‰¯å¥½ï¼Œæ˜“äºä¿®æ”¹ |

**æ€»ä½“è¯„ä»·**: ä»£ç è´¨é‡è‰¯å¥½ï¼Œä¸»è¦é—®é¢˜æ˜¯ç¯å¢ƒé…ç½®ï¼ˆä¾èµ–ç¼ºå¤±å’ŒGPUä¸å¯ç”¨ï¼‰

---

## ğŸ”§ å¿«é€Ÿä¿®å¤æ¸…å•

### å¿…é¡»ä¿®å¤ï¼ˆå¦åˆ™æ— æ³•è¿è¡Œï¼‰
- [ ] å®‰è£…transformersåº“: `pip install transformers`
- [ ] éªŒè¯å®‰è£…: `python -c "import transformers; print(transformers.__version__)"`

### å¼ºçƒˆå»ºè®®ä¿®å¤
- [ ] è®¾ç½®éšæœºç§å­ï¼ˆæ·»åŠ åˆ°Cell 1ï¼‰
- [ ] é…ç½®GPUç¯å¢ƒï¼ˆäº‘GPUæˆ–æœ¬åœ°CUDAï¼‰

### å¯é€‰ä¼˜åŒ–
- [ ] æ·»åŠ è®­ç»ƒæ—¥å¿—ï¼ˆæ—¶é—´ã€é€Ÿåº¦ï¼‰
- [ ] ä¿å­˜è®­ç»ƒå†å²å¹¶å¯è§†åŒ–
- [ ] è°ƒæ•´batch sizeï¼ˆæ ¹æ®ç¡¬ä»¶ï¼‰

---

## ğŸš€ ä¿®å¤åçš„å¯åŠ¨æ­¥éª¤

1. **å®‰è£…ä¾èµ–**
```bash
pip install transformers
```

2. **éªŒè¯ç¯å¢ƒ**
```bash
python -c "
import torch
import transformers
print(f'PyTorch: {torch.__version__}')
print(f'Transformers: {transformers.__version__}')
print(f'CUDA: {torch.cuda.is_available()}')
"
```

3. **æ·»åŠ éšæœºç§å­**ï¼ˆå¯é€‰ä½†æ¨èï¼‰
- åœ¨notebook Cell 1æœ€åæ·»åŠ seedè®¾ç½®ä»£ç 

4. **é€‰æ‹©è®­ç»ƒæ–¹å¼**
- æœ‰GPU: ç›´æ¥è¿è¡Œnotebook
- æ— GPUä½†ä¸æ€¥: CPUè®­ç»ƒï¼ˆé¢„è®¡10-20å°æ—¶ï¼‰
- æ— GPUä¸”æƒ³å¿«é€ŸéªŒè¯: ä¸Šä¼ åˆ°Google Colabä½¿ç”¨å…è´¹GPU

5. **å¼€å§‹è®­ç»ƒ**
```bash
jupyter notebook notebooks/04_BERT_Finetune.ipynb
```

---

## ğŸ“ åç»­å»ºè®®

### è®­ç»ƒå®Œæˆå
1. è¿è¡Œé”™è¯¯åˆ†ænotebook: `05_error_analysis.ipynb`
2. ç¡®ä¿æ›´æ–°å…¶ä¸­çš„`max_length=512`ä»¥ä¿æŒä¸€è‡´
3. å¯¹æ¯”æ–°æ—§æ¨¡å‹æ€§èƒ½

### è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘
1. å°è¯•ä¸åŒå­¦ä¹ ç‡: `1e-5`, `3e-5`
2. è°ƒæ•´dropout: `0.1`, `0.2`, `0.4`
3. å°è¯•æ›´å¤§çš„æ¨¡å‹: `bert-base-uncased`ï¼ˆå¦‚æœGPUå†…å­˜è¶³å¤Ÿï¼‰
4. ä½¿ç”¨focal losså¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯é€‰ï¼‰

---

## è”ç³»ä¿¡æ¯
å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. GitHub Issues: [é¡¹ç›®åœ°å€]
2. HuggingFaceæ–‡æ¡£: https://huggingface.co/docs/transformers
3. PyTorchè®ºå›: https://discuss.pytorch.org

---

**å®¡æŸ¥å®Œæˆæ—¶é—´**: 2025-12-11
**å®¡æŸ¥è€…**: Claude Code
**ä»£ç ç‰ˆæœ¬**: æ”¹è¿›åï¼ˆmax_length=512, lr=2e-5, cosine scheduler, early stoppingï¼‰
