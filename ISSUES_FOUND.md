# ä»£ç é—®é¢˜æ£€æŸ¥æ€»ç»“

## ğŸ“‹ æ£€æŸ¥å®Œæˆæ—¶é—´
2025-12-11

## ğŸ” æ£€æŸ¥ç»“æœæ¦‚è§ˆ

### å‘ç°çš„é—®é¢˜ç»Ÿè®¡
- ğŸ”´ **ä¸¥é‡é—®é¢˜**: 2ä¸ªï¼ˆå¿…é¡»ä¿®å¤ï¼‰
- âš ï¸ **é‡è¦é—®é¢˜**: 2ä¸ªï¼ˆå¼ºçƒˆå»ºè®®ä¿®å¤ï¼‰
- ğŸ’¡ **ä¼˜åŒ–å»ºè®®**: 3ä¸ªï¼ˆå¯é€‰ï¼‰
- âœ… **ä»£ç ä¼˜ç§€**: 5ä¸ªæ–¹é¢

---

## ğŸ”´ ä¸¥é‡é—®é¢˜ï¼ˆé˜»å¡æ€§ï¼‰

### é—®é¢˜1: Transformersåº“æœªå®‰è£…
**ä¸¥é‡ç¨‹åº¦**: P0 - é˜»å¡è¿è¡Œ

**ç°è±¡**:
```bash
ModuleNotFoundError: No module named 'transformers'
```

**å½±å“**:
- âŒ æ— æ³•å¯¼å…¥BERTæ¨¡å‹å’Œtokenizer
- âŒ Notebookç¬¬ä¸€ä¸ªcellå°±ä¼šå¤±è´¥
- âŒ å®Œå…¨æ— æ³•å¼€å§‹è®­ç»ƒ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¿«é€Ÿä¿®å¤
pip install transformers

# æˆ–è¿è¡Œä¿®å¤è„šæœ¬
./fix_environment.sh
```

**éªŒè¯**:
```python
import transformers
print(transformers.__version__)
# åº”è¾“å‡º: 4.x.x
```

---

### é—®é¢˜2: GPUä¸å¯ç”¨ï¼Œè®­ç»ƒå°†éå¸¸æ…¢
**ä¸¥é‡ç¨‹åº¦**: P1 - ä¸¥é‡å½±å“æ•ˆç‡

**ç°è±¡**:
```python
torch.cuda.is_available()  # è¿”å›False
```

**å½±å“**:
- âš ï¸ è®­ç»ƒé€Ÿåº¦é™ä½**50-100å€**
- âš ï¸ max_length=512æ—¶ï¼Œå•epoché¢„è®¡**1-2å°æ—¶**
- âš ï¸ 10ä¸ªepochå¯èƒ½éœ€è¦**10-20å°æ—¶**

**å½“å‰ç¯å¢ƒ**:
- ç³»ç»Ÿ: macOS Darwin 24.6.0
- PyTorch: 2.5.1 (CPUç‰ˆæœ¬)
- CUDA: ä¸å¯ç”¨

**è§£å†³æ–¹æ¡ˆï¼ˆé€‰æ‹©ä¸€ä¸ªï¼‰**:

**æ–¹æ¡ˆA: ä½¿ç”¨äº‘GPUï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰**
```
Google Colab (å…è´¹):
1. è®¿é—® https://colab.research.google.com
2. ä¸Šä¼  04_BERT_Finetune.ipynb
3. è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ T4 GPU
4. è¿è¡Œå…¨éƒ¨å•å…ƒæ ¼

Kaggle (å…è´¹):
1. è®¿é—® https://www.kaggle.com
2. åˆ›å»ºæ–°notebook
3. è®¾ç½® â†’ Accelerator â†’ GPU P100
4. ä¸Šä¼ ä»£ç å’Œæ•°æ®
```

**æ–¹æ¡ˆB: å®‰è£…CUDAç‰ˆPyTorchï¼ˆå¦‚æœæœ‰NVIDIA GPUï¼‰**
```bash
# æ£€æŸ¥æ˜¯å¦æœ‰NVIDIA GPU
nvidia-smi

# å¦‚æœæœ‰ï¼Œå¸è½½CPUç‰ˆPyTorch
pip uninstall torch torchvision torchaudio

# å®‰è£…CUDA 11.8ç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# éªŒè¯
python -c "import torch; print(torch.cuda.is_available())"
```

**æ–¹æ¡ˆC: é™ä½å‚æ•°åœ¨CPUä¸Šè®­ç»ƒï¼ˆä¸æ¨èï¼Œä»…ç”¨äºæµ‹è¯•ï¼‰**
```python
# ä¸´æ—¶é™ä½å‚æ•°éªŒè¯ä»£ç é€»è¾‘
max_length = 128  # ä»512é™ä½
batch_size = 4    # ä»16é™ä½
num_epochs = 2    # ä»10é™ä½
```

---

## âš ï¸ é‡è¦é—®é¢˜ï¼ˆå½±å“è´¨é‡ï¼‰

### é—®é¢˜3: ç¼ºå°‘éšæœºç§å­è®¾ç½®
**ä¸¥é‡ç¨‹åº¦**: P2 - å½±å“å¯å¤ç°æ€§

**é—®é¢˜æè¿°**:
- æ¯æ¬¡è®­ç»ƒç»“æœä¼šæœ‰éšæœºæ³¢åŠ¨
- æ— æ³•å‡†ç¡®å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ
- è®ºæ–‡/æŠ¥å‘Šä¸­çš„ç»“æœä¸å¯å¤ç°

**å·²ä¿®å¤**: âœ…

ä¿®æ”¹äº† `notebooks/04_BERT_Finetune.ipynb` Cell 1ï¼Œæ·»åŠ äº†ï¼š
```python
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(42)
```

**æ•ˆæœ**:
- âœ… ç›¸åŒé…ç½®ä¸‹ç»“æœå®Œå…¨ä¸€è‡´
- âœ… ä¾¿äºè°ƒè¯•å’Œå¯¹æ¯”
- âœ… ç¬¦åˆå­¦æœ¯è§„èŒƒ

---

### é—®é¢˜4: æ•°æ®é¢„å¤„ç†ä¸ä¸€è‡´ï¼ˆå·²åœ¨ä¹‹å‰ä¿®å¤ï¼‰
**ä¸¥é‡ç¨‹åº¦**: P2 - å·²ä¿®å¤ âœ…

**åŸé—®é¢˜**:
- è®­ç»ƒæ—¶ä½¿ç”¨äº†`basic_clean(text)`
- BERTåº”è¯¥ä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼ˆä»…å»ç©ºæ ¼ï¼‰

**å½“å‰çŠ¶æ€**: å·²ä¿®å¤
```python
# æ­£ç¡®çš„åšæ³• âœ…
train_texts_clean = [text.strip() for text in train_texts]
```

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼ˆå¯é€‰ï¼‰

### å»ºè®®1: æ·»åŠ è®­ç»ƒè¿›åº¦ç›‘æ§
**ä¼˜å…ˆçº§**: P3 - æå‡ç”¨æˆ·ä½“éªŒ

**å»ºè®®ä»£ç **:
```python
import time

# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
for epoch in range(num_epochs):
    start_time = time.time()

    # ... è®­ç»ƒä»£ç  ...

    elapsed = time.time() - start_time
    print(f"  Epoch Time: {elapsed/60:.2f} min")
    print(f"  Samples/sec: {len(train_dataset)/elapsed:.2f}")
```

**å¥½å¤„**:
- å¯ä»¥ä¼°ç®—å‰©ä½™è®­ç»ƒæ—¶é—´
- ç›‘æ§è®­ç»ƒé€Ÿåº¦å˜åŒ–
- å‘ç°æ€§èƒ½ç“¶é¢ˆ

---

### å»ºè®®2: ä¿å­˜å¹¶å¯è§†åŒ–è®­ç»ƒå†å²
**ä¼˜å…ˆçº§**: P3 - ä¾¿äºåˆ†æ

**å»ºè®®ä»£ç **:
```python
import matplotlib.pyplot as plt

history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

# è®­ç»ƒå¾ªç¯ä¸­æ”¶é›†æ•°æ®
history['train_loss'].append(train_loss)
# ... å…¶ä»–æŒ‡æ ‡ ...

# è®­ç»ƒç»“æŸåç»˜å›¾
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train')
plt.plot(history['val_loss'], label='Val')
plt.legend(); plt.title('Loss')

plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train')
plt.plot(history['val_acc'], label='Val')
plt.legend(); plt.title('Accuracy')
plt.show()
```

**å¥½å¤„**:
- ç›´è§‚çœ‹åˆ°è®­ç»ƒè¿‡ç¨‹
- å‘ç°è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ
- è¾…åŠ©è°ƒå‚å†³ç­–

---

### å»ºè®®3: è°ƒæ•´Batch Sizeï¼ˆæ ¹æ®ç¡¬ä»¶ï¼‰
**ä¼˜å…ˆçº§**: P3 - æ€§èƒ½ä¼˜åŒ–

**å½“å‰é…ç½®**:
```python
batch_size = 16
```

**å»ºè®®**:
| ç¡¬ä»¶æ¡ä»¶ | æ¨èBatch Size | è¯´æ˜ |
|----------|----------------|------|
| CPUè®­ç»ƒ | 8 | é™ä½å†…å­˜å‹åŠ› |
| GPU â‰¤6GB | 16 | å½“å‰é…ç½® |
| GPU 8-12GB | 32 | å¯æå‡è®­ç»ƒé€Ÿåº¦ |
| GPU â‰¥16GB | 64 | å……åˆ†åˆ©ç”¨GPU |

**ä¿®æ”¹æ–¹å¼**:
```python
# æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒæ•´
if not torch.cuda.is_available():
    batch_size = 8  # CPU
elif torch.cuda.get_device_properties(0).total_memory > 12e9:
    batch_size = 32  # å¤§æ˜¾å­˜GPU
else:
    batch_size = 16  # é»˜è®¤
```

---

## âœ… ä»£ç è´¨é‡ä¼˜ç§€çš„åœ°æ–¹

### 1. æ¨¡å—åŒ–è®¾è®¡ä¼˜ç§€ â­â­â­â­â­
- `src/model/bert_model.py`: æ¨¡å‹å®šä¹‰æ¸…æ™°
- `src/train_nn.py`: è®­ç»ƒé€»è¾‘å¤ç”¨æ€§å¼º
- `src/data_utils.py`: æ•°æ®åŠ è½½ç»Ÿä¸€æ¥å£
- `src/evaluate.py`: è¯„ä¼°æŒ‡æ ‡å®Œæ•´

### 2. è®­ç»ƒå‡½æ•°å®ç°è§„èŒƒ â­â­â­â­â­
```python
# src/train_nn.py
- âœ… æ¢¯åº¦è£å‰ª (clip_grad_norm_)
- âœ… Scheduleré€batchæ›´æ–°
- âœ… æ­£ç¡®ä½¿ç”¨ model.train() / model.eval()
- âœ… è¯„ä¼°æ—¶ä½¿ç”¨ torch.no_grad()
```

### 3. BERTæ¨¡å‹å®ç°æ ‡å‡† â­â­â­â­â­
```python
# src/model/bert_model.py
- âœ… ä½¿ç”¨[CLS] tokenè¡¨ç¤º (outputs.last_hidden_state[:, 0, :])
- âœ… Dropoutæ­£ç¡®åº”ç”¨
- âœ… æ”¯æŒfreeze_bertå‚æ•°
- âœ… è‡ªåŠ¨è·å–hidden_size
```

### 4. Early Stoppingé€»è¾‘æ­£ç¡® â­â­â­â­â­
```python
# notebooks/04_BERT_Finetune.ipynb - Cell 17
- âœ… æ­£ç¡®ä¿å­˜æœ€ä½³æ¨¡å‹
- âœ… Patienceæœºåˆ¶åˆç†
- âœ… è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
```

### 5. æ•°æ®å¤„ç†å¥å£® â­â­â­â­
```python
# src/data_utils.py
- âœ… å¤„ç†ç¼ºå¤±å€¼ (fillna)
- âœ… è‡ªåŠ¨åˆ›å»ºlabelæ˜ å°„
- âœ… è·¯å¾„å¤„ç†è·¨å¹³å°å…¼å®¹
```

---

## ğŸ“Š é—®é¢˜ä¿®å¤è¿›åº¦

| é—®é¢˜ | ä¸¥é‡ç¨‹åº¦ | çŠ¶æ€ | è¯´æ˜ |
|------|----------|------|------|
| Transformersç¼ºå¤± | ğŸ”´ P0 | â³ å¾…ä¿®å¤ | éœ€è¦: `pip install transformers` |
| GPUä¸å¯ç”¨ | ğŸ”´ P1 | â³ å¾…ä¿®å¤ | å»ºè®®ä½¿ç”¨äº‘GPU |
| ç¼ºå°‘éšæœºç§å­ | âš ï¸ P2 | âœ… å·²ä¿®å¤ | Cell 1å·²æ·»åŠ  |
| æ•°æ®é¢„å¤„ç† | âš ï¸ P2 | âœ… å·²ä¿®å¤ | ä½¿ç”¨text.strip() |
| è®­ç»ƒç›‘æ§ | ğŸ’¡ P3 | ğŸ“ å»ºè®® | å¯é€‰ä¼˜åŒ– |
| å†å²å¯è§†åŒ– | ğŸ’¡ P3 | ğŸ“ å»ºè®® | å¯é€‰ä¼˜åŒ– |
| Batch Size | ğŸ’¡ P3 | ğŸ“ å»ºè®® | å¯é€‰ä¼˜åŒ– |

---

## ğŸš€ å¿«é€Ÿä¿®å¤æŒ‡å—

### Step 1: å®‰è£…ä¾èµ–ï¼ˆå¿…é¡»ï¼‰
```bash
# è¿è¡Œä¿®å¤è„šæœ¬
cd /Users/bestalex/Desktop/000_ai_support_tickets_tuo
./fix_environment.sh

# æˆ–æ‰‹åŠ¨å®‰è£…
pip install transformers
```

### Step 2: éªŒè¯ç¯å¢ƒï¼ˆå¿…é¡»ï¼‰
```bash
python -c "
import torch
import transformers
print(f'PyTorch: {torch.__version__}')
print(f'Transformers: {transformers.__version__}')
print(f'CUDA: {torch.cuda.is_available()}')
"
```

**æœŸæœ›è¾“å‡º**:
```
PyTorch: 2.5.1
Transformers: 4.x.x
CUDA: True (å¦‚æœæœ‰GPU) æˆ– False (éœ€è¦ä½¿ç”¨äº‘GPU)
```

### Step 3: é€‰æ‹©è®­ç»ƒæ–¹å¼

**é€‰é¡¹A: æœ¬åœ°æœ‰GPU**
```bash
jupyter notebook notebooks/04_BERT_Finetune.ipynb
# ç›´æ¥è¿è¡Œå…¨éƒ¨cell
```

**é€‰é¡¹B: æœ¬åœ°æ— GPUï¼ˆæ¨èäº‘å¹³å°ï¼‰**
```
1. è®¿é—® https://colab.research.google.com
2. æ–‡ä»¶ â†’ ä¸Šä¼ ç¬”è®°æœ¬ â†’ é€‰æ‹© 04_BERT_Finetune.ipynb
3. ä¸Šä¼  data/ å’Œ src/ æ–‡ä»¶å¤¹
4. è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU
5. è¿è¡Œ â†’ å…¨éƒ¨è¿è¡Œ
```

**é€‰é¡¹C: æœ¬åœ°CPUï¼ˆä»…ç”¨äºéªŒè¯ï¼‰**
```python
# åœ¨notebookä¸­ä¸´æ—¶é™ä½å‚æ•°
max_length = 128
batch_size = 8
num_epochs = 2
```

### Step 4: å¼€å§‹è®­ç»ƒ
- è¿è¡Œnotebookæ‰€æœ‰å•å…ƒæ ¼
- é¢„è®¡æ—¶é—´ï¼ˆGPUï¼‰: 30-60åˆ†é’Ÿ
- é¢„è®¡æ—¶é—´ï¼ˆCPUï¼‰: 10-20å°æ—¶ âš ï¸

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ

### å½“å‰æ€§èƒ½
- å‡†ç¡®ç‡: 64.43%
- F1 Macro: 0.625
- é—®é¢˜: ä½äºLogistic RegressionåŸºçº¿

### ä¿®å¤åé¢„æœŸæ€§èƒ½
- å‡†ç¡®ç‡: **68-72%** â¬†ï¸ +4-8%
- F1 Macro: **0.66-0.70** â¬†ï¸ +0.04-0.08
- æ•ˆæœ: æ˜æ˜¾è¶…è¶Šä¼ ç»ŸåŸºçº¿

### æ”¹è¿›æ¥æº
1. max_length: 256â†’512 (å‡å°‘æˆªæ–­) â‰ˆ +2-3%
2. è®­ç»ƒè½®æ•°: 3â†’10 (å……åˆ†è®­ç»ƒ) â‰ˆ +2-3%
3. å­¦ä¹ ç‡è°ƒåº¦: linearâ†’cosine â‰ˆ +1-2%
4. å­¦ä¹ ç‡: 5e-5â†’2e-5 (æ›´ç¨³å®š) â‰ˆ +1%
5. Early Stopping (é˜²è¿‡æ‹Ÿåˆ) â‰ˆ +0.5-1%

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

### å¸¸è§é—®é¢˜

**Q: transformerså®‰è£…å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**
```bash
# å°è¯•å‡çº§pip
pip install --upgrade pip

# ä½¿ç”¨å›½å†…é•œåƒ
pip install transformers -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**Q: å¦‚ä½•åœ¨Colabä¸Šè¿è¡Œï¼Ÿ**
1. ä¸Šä¼ notebookå’Œæ•°æ®
2. ä¿®æ”¹è·¯å¾„ï¼ˆColabæ ¹ç›®å½•ä¸åŒï¼‰
3. é€‰æ‹©GPUè¿è¡Œæ—¶
4. é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆçº¦500MBï¼‰

**Q: CPUè®­ç»ƒå¤ªæ…¢å¯ä»¥ä¸­æ–­å—ï¼Ÿ**
- å¯ä»¥ï¼Early Stoppingä¼šä¿å­˜æœ€ä½³æ¨¡å‹
- ä½†å»ºè®®è‡³å°‘è¿è¡Œ3-5ä¸ªepoch

**Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
```python
# é™ä½batch_size
batch_size = 8  # æˆ–4

# é™ä½max_length
max_length = 256  # æˆ–128
```

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

- `CODE_REVIEW.md`: è¯¦ç»†ä»£ç å®¡æŸ¥æŠ¥å‘Š
- `BERT_IMPROVEMENTS.md`: BERTæ”¹è¿›æ–¹æ¡ˆè¯´æ˜
- `fix_environment.sh`: ç¯å¢ƒä¿®å¤è„šæœ¬
- `notebooks/04_BERT_Finetune.ipynb`: æ”¹è¿›åçš„è®­ç»ƒnotebook

---

**æœ€åæ›´æ–°**: 2025-12-11
**çŠ¶æ€**: âœ… ä»£ç æ”¹è¿›å®Œæˆï¼Œâ³ ç­‰å¾…ç¯å¢ƒä¿®å¤å’Œè®­ç»ƒ
